{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 11511031,
          "sourceType": "datasetVersion",
          "datasetId": 7218046
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MBJM\n",
        "\n",
        "## RAG-based Spoiler Detection and Context-Preserving Redaction\n",
        "\n",
        "\n",
        "**Names & SRNs of the team:**\n",
        "\n",
        "Hamsini V & PES1UG22AM062\n",
        "\n",
        "Kirti S & PES1UG22AM084\n",
        "\n",
        "Sudarshan Srinivasan & PES1UG22AM166"
      ],
      "metadata": {
        "id": "8-nW8RmXBaGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "-ikG4LC4BaGf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers chromadb -q"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T08:10:50.758421Z",
          "iopub.execute_input": "2025-04-22T08:10:50.758656Z",
          "iopub.status.idle": "2025-04-22T08:12:29.460420Z",
          "shell.execute_reply.started": "2025-04-22T08:10:50.758633Z",
          "shell.execute_reply": "2025-04-22T08:12:29.459651Z"
        },
        "id": "ycpgb5n1BaGg",
        "outputId": "a188772c-2f12-4054-d08d-a7a640a096fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngoogle-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Snippet 1: Setup and Configuration (for Index Creation) ---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This snippet initializes the environment for building a ChromaDB vector index from source data. Its primary role is to import necessary libraries and define crucial configuration parameters, such as file paths, model identifiers, and processing settings, that will be used in subsequent steps to load data, generate embeddings, and save the index.\n",
        "\n",
        "## Key Actions\n",
        "\n",
        "1.  **Import Libraries:** Imports essential Python modules:\n",
        "    *   `json`: To load data from JSON files (presumably containing the spoiler text and metadata).\n",
        "    *   `os`: For interacting with the operating system, particularly for path manipulation (e.g., joining paths, checking existence - though not explicitly used *in* this snippet, likely needed later).\n",
        "    *   `glob`: To find files matching a specific pattern (e.g., finding all `.json` files within the `SOURCE_DATA_PATH`).\n",
        "    *   `chromadb`: The client library for creating and interacting with the Chroma vector database.\n",
        "    *   `SentenceTransformer` (from `sentence_transformers`): The class used to load models that convert text into dense vector embeddings.\n",
        "    *   `time`: Standard library for time-related functions, potentially used for timing the index creation process.\n",
        "    *   `numpy`: A fundamental package for scientific computing, often used implicitly by embedding models or for handling numerical data (added here for potential later use, though not directly used in *this* snippet).\n",
        "\n",
        "2.  **Define Configuration Variables:** Sets up key parameters controlling the index creation process:\n",
        "    *   `SOURCE_DATA_PATH` (str): Specifies the directory path (`\"/kaggle/input/thewirechunked2/\"`) where the input JSON files containing the text data (likely \"The Wire\" spoilers, based on context) are located. In Kaggle, this points to a read-only input dataset.\n",
        "    *   `MODEL_NAME` (str): Defines the identifier (`'all-MiniLM-L6-v2'`) for the pre-trained Sentence Transformer model to be used for generating text embeddings. The `SentenceTransformer` library will typically download this model from Hugging Face Hub if it's not available locally or specified via a local path. `'all-MiniLM-L6-v2'` is a popular choice known for its balance of speed and embedding quality.\n",
        "        *   *Note:* A commented-out line (`# MODEL_PATH = ...`) indicates an alternative approach where the model could be loaded from a pre-downloaded directory within Kaggle input, potentially saving download time if the model files are provided as a separate dataset. The active configuration uses `MODEL_NAME`, implying direct download/use via the library.\n",
        "    *   `OUTPUT_DB_PATH` (str): Sets the directory path (`\"/kaggle/working/the_wire_s1_chroma_db_3\"`) where the newly created ChromaDB persistent index will be stored. In Kaggle, `/kaggle/working/` is the primary writable directory. ChromaDB will create necessary files within this directory.\n",
        "    *   `COLLECTION_NAME` (str): Assigns a name (`\"wire_s1_spoilers_3\"`) to the specific collection within the ChromaDB database. This acts like a table name and is essential for storing and later retrieving the data associated with this specific index.\n",
        "    *   `BATCH_SIZE` (int): Determines how many text documents will be processed (i.e., embedded) in a single batch (`128`). This parameter affects performance and memory usage, especially GPU VRAM if running on a GPU. Larger batches can increase throughput but require more memory.\n",
        "\n",
        "## Context & Importance\n",
        "\n",
        "This snippet is the **foundational step** for the **index creation workflow**. It doesn't perform any data loading, embedding, or saving itself, but it meticulously defines *all* the necessary parameters required for those subsequent actions. The choices made here (source data location, embedding model, output path, collection name, batch size) directly dictate how the index will be built and what characteristics it will have. Ensuring these paths and names are correct is critical for the success of the following steps (loading data, creating embeddings, initializing ChromaDB, and adding documents)."
      ],
      "metadata": {
        "id": "-oAWZyhwBaGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Snippet 1: Setup and Configuration ===\n",
        "\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import time\n",
        "import numpy as np # Added for potential use later\n",
        "\n",
        "print(\"--- Running Snippet 1: Setup and Configuration ---\")\n",
        "\n",
        "# --- Configuration ---\n",
        "# Path to the directory containing your JSON spoiler files\n",
        "SOURCE_DATA_PATH = \"/kaggle/input/thewirechunked2/\"\n",
        "# Path to the pre-loaded sentence transformer model directory\n",
        "# IMPORTANT: Adjust this path if your Kaggle input dataset for the model has a different name!\n",
        "#MODEL_PATH = \"/kaggle/input/sentencetransformers-allminilml6v2/all-MiniLM-L6-v2\"\n",
        "MODEL_NAME = 'all-MiniLM-L6-v2' # Use this line instead if you want to download the model directly\n",
        "\n",
        "# Path where the ChromaDB index will be saved in the notebook's temporary output\n",
        "OUTPUT_DB_PATH = \"/kaggle/working/the_wire_s1_chroma_db_3\"\n",
        "# Name for the collection within ChromaDB\n",
        "COLLECTION_NAME = \"wire_s1_spoilers_3\"\n",
        "# Batch size for processing embeddings (adjust based on GPU memory if needed)\n",
        "BATCH_SIZE = 128\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T08:12:49.386966Z",
          "iopub.execute_input": "2025-04-22T08:12:49.387661Z",
          "iopub.status.idle": "2025-04-22T08:13:16.061481Z",
          "shell.execute_reply.started": "2025-04-22T08:12:49.387630Z",
          "shell.execute_reply": "2025-04-22T08:13:16.060814Z"
        },
        "id": "RiAruDRgBaGg",
        "outputId": "f1dfa390-cfa6-42d3-a196-2be852242d2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-04-22 08:13:02.942596: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745309583.130657      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745309583.186744      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "--- Running Snippet 1: Setup and Configuration ---\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Source Data Path: {SOURCE_DATA_PATH}\")\n",
        "print(f\"Embedding Model Path: {MODEL_NAME}\")\n",
        "print(f\"Output DB Path: {OUTPUT_DB_PATH}\")\n",
        "print(f\"Collection Name: {COLLECTION_NAME}\")\n",
        "print(\"Configuration set.\")\n",
        "print(\"--- Finished Snippet 1 ---\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T08:13:53.789793Z",
          "iopub.execute_input": "2025-04-22T08:13:53.790875Z",
          "iopub.status.idle": "2025-04-22T08:13:53.795287Z",
          "shell.execute_reply.started": "2025-04-22T08:13:53.790853Z",
          "shell.execute_reply": "2025-04-22T08:13:53.794570Z"
        },
        "id": "aYo01VBzBaGh",
        "outputId": "68fb0d8b-1f29-455f-f070-2295cbca9f7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Source Data Path: /kaggle/input/thewirechunked2/\nEmbedding Model Path: all-MiniLM-L6-v2\nOutput DB Path: /kaggle/working/the_wire_s1_chroma_db_3\nCollection Name: wire_s1_spoilers_3\nConfiguration set.\n--- Finished Snippet 1 ---\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Snippet 2: Load Spoiler Data ---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This snippet is responsible for locating, reading, and aggregating data from multiple JSON files containing spoiler information. It assumes these files are organized within a specific directory (`SOURCE_DATA_PATH`) and potentially follow a naming convention that allows for logical sorting (e.g., by episode number). The final output is a single Python list (`all_spoiler_data`) containing all the data points loaded from the individual files, ready for further processing (like embedding generation).\n",
        "\n",
        "## Key Actions\n",
        "\n",
        "1.  **Import Necessary Libraries:** Imports `json` (for parsing JSON), `os` (for path manipulation like joining paths and extracting basenames), and `glob` (for finding files matching a pattern).\n",
        "2.  **Configuration Display:** Prints the `SOURCE_DATA_PATH` to confirm the location being searched.\n",
        "3.  **Define Search Pattern:**\n",
        "    *   Constructs a file search pattern (`json_pattern`) using `os.path.join` to combine the `SOURCE_DATA_PATH` with a specific filename pattern (`\"ragdatas1e*.json\"`). This pattern aims to find JSON files specifically related to Season 1 episodes.\n",
        "    *   Uses `glob.glob(json_pattern)` to find all file paths within the source directory that match this pattern.\n",
        "4.  **File Discovery Check:**\n",
        "    *   Checks if the `json_files` list is empty.\n",
        "    *   If no files are found, it prints an error message indicating the pattern used and suggests checking the path and naming convention, preventing further execution on empty data.\n",
        "5.  **Sort Files:**\n",
        "    *   If files are found, it prints the number discovered.\n",
        "    *   Defines a `sort_key` helper function to extract the episode number from the filename (assuming the format `ragdatas1eXX.json`). It includes basic error handling to place files with unexpected names at the end.\n",
        "    *   Sorts the list of discovered `json_files` using this `sort_key` to ensure files are processed in episode order (e.g., `e01.json` before `e02.json`). This can be important for preserving logical sequence if the data has temporal dependencies.\n",
        "    *   Prints the sorted list of filenames that will be processed.\n",
        "6.  **Iterate and Load Data:**\n",
        "    *   Loops through each `file_path` in the `sorted_json_files` list.\n",
        "    *   Prints the name of the file currently being processed.\n",
        "    *   Uses a `try...except` block for robust file handling:\n",
        "        *   Opens the file in read mode (`with open(file_path, 'r') as f:`).\n",
        "        *   Parses the JSON content using `json.load(f)`.\n",
        "        *   **Data Structure Validation:** Checks if the loaded `episode_data` is a Python `list` (as expected). If it is, it appends all items from this list to the main `all_spoiler_data` list using `extend()`.\n",
        "        *   If the data structure is not a list, it prints a warning and skips the file.\n",
        "        *   Catches `json.JSONDecodeError` specifically for issues during parsing.\n",
        "        *   Catches general `Exception` for any other errors during file processing (e.g., file not found if it was deleted between `glob` and `open`, permission issues).\n",
        "7.  **Report Summary:**\n",
        "    *   After the loop, prints the total number of individual spoiler events loaded into `all_spoiler_data` and the total number of files processed.\n",
        "8.  **Basic Validation:**\n",
        "    *   Performs a final check to see if `all_spoiler_data` actually contains any data. Prints an error if it's empty, confirming a potential issue upstream. Prints a success message otherwise.\n",
        "\n",
        "## Context & Importance\n",
        "\n",
        "This snippet acts as the data ingestion stage for the index creation pipeline. It bridges the gap between raw data stored in files and a structured Python representation (`all_spoiler_data`) needed for the next steps.\n",
        "*   **Data Aggregation:** It gathers potentially fragmented data (one file per episode) into a unified list.\n",
        "*   **Robustness:** Includes error handling for missing files, malformed JSON, and unexpected data structures within files.\n",
        "*   **Order Preservation:** The sorting step ensures that data is loaded in a predictable, logical sequence (by episode), which might be relevant depending on how the data is structured and used later.\n",
        "*   **Foundation:** Successfully loading and structuring the data here is **critical** for the subsequent embedding generation and database population steps. If this snippet fails or loads incorrect data, the resulting vector index will be flawed."
      ],
      "metadata": {
        "id": "okdeLcz_BaGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Snippet 2: Load Spoiler Data ===\n",
        "\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Configuration from Snippet 1 (repeated for clarity, not strictly needed if run in same session)\n",
        "SOURCE_DATA_PATH = \"/kaggle/input/thewirechunked2/\"\n",
        "\n",
        "print(\"--- Running Snippet 2: Load Spoiler Data ---\")\n",
        "print(f\"Loading JSON data from: {SOURCE_DATA_PATH}\")\n",
        "\n",
        "all_spoiler_data = []\n",
        "# Construct the search pattern based on the user's file names\n",
        "json_pattern = os.path.join(SOURCE_DATA_PATH, \"ragdatas1e*.json\")\n",
        "json_files = glob.glob(json_pattern)\n",
        "\n",
        "if not json_files:\n",
        "    print(f\"Error: No JSON files found matching pattern '{json_pattern}'.\")\n",
        "    print(\"Please check the SOURCE_DATA_PATH and file naming convention.\")\n",
        "else:\n",
        "    print(f\"Found {len(json_files)} JSON files matching pattern.\")\n",
        "    # Sort files numerically based on episode number if possible\n",
        "    def sort_key(filepath):\n",
        "        basename = os.path.basename(filepath)\n",
        "        # Extract episode number (assuming format ragdatas1eXX.json)\n",
        "        try:\n",
        "            num_part = basename.split('e')[1].split('.')[0]\n",
        "            return int(num_part)\n",
        "        except:\n",
        "            return 999 # Put files that don't match at the end\n",
        "\n",
        "    sorted_json_files = sorted(json_files, key=sort_key)\n",
        "    print(\"Files will be processed in this order:\")\n",
        "    for f in sorted_json_files:\n",
        "        print(f\"  - {os.path.basename(f)}\")\n",
        "\n",
        "    for file_path in sorted_json_files:\n",
        "        print(f\"  Processing file: {os.path.basename(file_path)}...\")\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                episode_data = json.load(f)\n",
        "                # episode_data should be the list of dictionaries for that episode\n",
        "                if isinstance(episode_data, list):\n",
        "                     all_spoiler_data.extend(episode_data) # Add this episode's list to the main list\n",
        "                else:\n",
        "                     print(f\"    Warning: Expected a list in {file_path}, but got {type(episode_data)}. Skipping.\")\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"    Error decoding JSON from file: {file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"    An error occurred processing file {file_path}: {e}\")\n",
        "\n",
        "print(f\"\\nLoaded total {len(all_spoiler_data)} spoiler events from {len(json_files)} files.\")\n",
        "\n",
        "# Basic validation\n",
        "if not all_spoiler_data:\n",
        "    print(\"Error: No data was loaded into all_spoiler_data. Check JSON files and paths.\")\n",
        "else:\n",
        "    print(\"Data loading appears successful.\")\n",
        "\n",
        "print(\"--- Finished Snippet 2 ---\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T08:14:07.134900Z",
          "iopub.execute_input": "2025-04-22T08:14:07.135194Z",
          "iopub.status.idle": "2025-04-22T08:14:07.188280Z",
          "shell.execute_reply.started": "2025-04-22T08:14:07.135173Z",
          "shell.execute_reply": "2025-04-22T08:14:07.187702Z"
        },
        "id": "U40qyBX0BaGh",
        "outputId": "3a7a2640-6f0f-40de-e178-00070af2eeb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "--- Running Snippet 2: Load Spoiler Data ---\nLoading JSON data from: /kaggle/input/thewirechunked2/\nFound 13 JSON files matching pattern.\nFiles will be processed in this order:\n  - ragdatas1e1.json\n  - ragdatas1e2.json\n  - ragdatas1e3.json\n  - ragdatas1e4.json\n  - ragdatas1e5.json\n  - ragdatas1e6.json\n  - ragdatas1e7.json\n  - ragdatas1e8.json\n  - ragdatas1e9.json\n  - ragdatas1e10.json\n  - ragdatas1e11.json\n  - ragdatas1e12.json\n  - ragdatas1e13.json\n  Processing file: ragdatas1e1.json...\n  Processing file: ragdatas1e2.json...\n  Processing file: ragdatas1e3.json...\n  Processing file: ragdatas1e4.json...\n  Processing file: ragdatas1e5.json...\n  Processing file: ragdatas1e6.json...\n  Processing file: ragdatas1e7.json...\n  Processing file: ragdatas1e8.json...\n  Processing file: ragdatas1e9.json...\n  Processing file: ragdatas1e10.json...\n  Processing file: ragdatas1e11.json...\n  Processing file: ragdatas1e12.json...\n  Processing file: ragdatas1e13.json...\n\nLoaded total 466 spoiler events from 13 files.\nData loading appears successful.\n--- Finished Snippet 2 ---\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Snippet 3: Load Embedding Model ---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This snippet focuses on loading the pre-trained Sentence Transformer model specified in the configuration. This model is essential for converting the textual spoiler data (loaded in Snippet 2) into numerical vector embeddings, which are required for storage and similarity search within the ChromaDB vector database.\n",
        "\n",
        "## Key Actions\n",
        "\n",
        "1.  **Import Necessary Class:** Imports the `SentenceTransformer` class from the `sentence_transformers` library.\n",
        "2.  **Confirm Model Identifier:** Uses the `MODEL_NAME` variable (`'all-MiniLM-L6-v2'`) defined in Snippet 1 to specify which model to load.\n",
        "    *   *Note:* It includes commented-out code (`# MODEL_PATH = ...` and the corresponding `SentenceTransformer` call) to show the alternative approach of loading the model from a specific local directory path, which might be used if the model files were provided as a Kaggle input dataset. The active code uses `MODEL_NAME`, implying the library will handle finding or downloading the model.\n",
        "3.  **Status Logging:** Prints messages indicating the start of the loading process and which model (`MODEL_NAME`) is being loaded. It also includes a notice that this might involve downloading model files if they are not already cached locally (requiring internet access in environments like Kaggle).\n",
        "4.  **Instantiate Model:**\n",
        "    *   Uses a `try...except` block for robust loading.\n",
        "    *   Attempts to create an instance of the model using `embedding_model = SentenceTransformer(MODEL_NAME, device='cuda')`.\n",
        "        *   `MODEL_NAME`: Tells the library which model configuration and weights to use (e.g., `'all-MiniLM-L6-v2'`). The library typically downloads these from the Hugging Face Hub if needed.\n",
        "        *   `device='cuda'`: **Crucially**, this argument instructs the library to load the model onto the available CUDA-enabled GPU. Performing embedding calculations on a GPU is significantly faster than on a CPU for large datasets.\n",
        "5.  **Error Handling:**\n",
        "    *   If any exception occurs during model loading (e.g., download error due to no internet, invalid model name, insufficient GPU memory, issues with underlying libraries like PyTorch/CUDA), the `except` block catches it.\n",
        "    *   Prints an informative error message including the exception details (`e`).\n",
        "    *   Provides hints for troubleshooting (check internet or the model path if that method was used).\n",
        "    *   Includes a commented-out `exit()` call, suggesting that in a full script, execution might be halted if the model fails to load, as subsequent steps depend on it.\n",
        "6.  **Success Confirmation:** If the model loads without errors, it prints a success message confirming the model is loaded onto the GPU.\n",
        "\n",
        "## Context & Importance\n",
        "\n",
        "This snippet is a critical prerequisite for creating the vector index.\n",
        "*   It loads the **engine (`embedding_model`)** responsible for converting text into meaningful numerical representations.\n",
        "*   Loading the model onto the **GPU (`'cuda'`)** is vital for achieving reasonable performance when processing potentially large amounts of text data in the next step.\n",
        "*   The success of this step directly impacts the ability to proceed with generating embeddings for the spoiler data loaded in Snippet 2. Failure here means the index cannot be built."
      ],
      "metadata": {
        "id": "mE7FLZtgBaGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Snippet 3: Load Embedding Model ===\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Configuration from Snippet 1 (using MODEL_NAME since MODEL_PATH wasn't a directory)\n",
        "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
        "# MODEL_PATH = \"/kaggle/input/sentencetransformers-allminilml6v2/all-MiniLM-L6-v2\" # Use this if you loaded from Kaggle input instead\n",
        "\n",
        "print(\"--- Running Snippet 3: Load Embedding Model ---\")\n",
        "# Make sure internet is enabled in Kaggle settings if using MODEL_NAME\n",
        "print(f\"Loading embedding model '{MODEL_NAME}'...\")\n",
        "print(\"(This may download the model files if not cached)...\")\n",
        "\n",
        "try:\n",
        "    # Use MODEL_NAME to download/load\n",
        "    embedding_model = SentenceTransformer(MODEL_NAME, device='cuda')\n",
        "    # If you decided to load from Kaggle input path instead, use this line:\n",
        "    # embedding_model = SentenceTransformer(MODEL_PATH, device='cuda')\n",
        "    print(\"Embedding model loaded successfully onto GPU.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading embedding model: {e}\")\n",
        "    print(\"Check internet connection (if downloading) or MODEL_PATH (if loading from input).\")\n",
        "    # Optionally add exit() here if you want the script to stop on error\n",
        "    # exit()\n",
        "\n",
        "print(\"--- Finished Snippet 3 ---\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T08:14:17.516434Z",
          "iopub.execute_input": "2025-04-22T08:14:17.516696Z",
          "iopub.status.idle": "2025-04-22T08:14:22.884737Z",
          "shell.execute_reply.started": "2025-04-22T08:14:17.516678Z",
          "shell.execute_reply": "2025-04-22T08:14:22.884098Z"
        },
        "colab": {
          "referenced_widgets": [
            "76b3f11ef0fc4cc1be18e3dfd7685c45",
            "8e21d891c8e544989559e81a27832b6b",
            "9814af7dc0604eca859e140955315365",
            "4f9247474037435ab6e80529137cfb63",
            "8ade93006c1a43c1a7205ba69e494fa8",
            "d70748a4608b4a41aaf7b663a083a769",
            "14e600f1263b40309f613820cb5a4fbe",
            "8b76009bdb514f7ab0a404a9debf2d97",
            "8360cc61eefd407bb9803489b6804f80",
            "85385a76539e483b90a775f6a80206c0",
            "6346e44db3a943f19b2baeb91cc23296"
          ]
        },
        "id": "jbOYBCp9BaGh",
        "outputId": "f9347d2e-3243-410b-d787-e7bc33d9dca0"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "--- Running Snippet 3: Load Embedding Model ---\nLoading embedding model 'all-MiniLM-L6-v2'...\n(This may download the model files if not cached)...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76b3f11ef0fc4cc1be18e3dfd7685c45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e21d891c8e544989559e81a27832b6b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9814af7dc0604eca859e140955315365"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f9247474037435ab6e80529137cfb63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ade93006c1a43c1a7205ba69e494fa8"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d70748a4608b4a41aaf7b663a083a769"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14e600f1263b40309f613820cb5a4fbe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b76009bdb514f7ab0a404a9debf2d97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8360cc61eefd407bb9803489b6804f80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85385a76539e483b90a775f6a80206c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6346e44db3a943f19b2baeb91cc23296"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Embedding model loaded successfully onto GPU.\n--- Finished Snippet 3 ---\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Snippet 4: Prepare Data for ChromaDB ---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This snippet takes the raw spoiler data loaded in Snippet 2 (a list of dictionaries, `all_spoiler_data`) and transforms it into the specific list formats required by ChromaDB for adding documents to a collection. These formats are: a list of text documents, a corresponding list of metadata dictionaries, and a corresponding list of unique document IDs.\n",
        "\n",
        "## Key Actions\n",
        "\n",
        "1.  **Prerequisite Check:**\n",
        "    *   Verifies that the `all_spoiler_data` variable exists (was created in Snippet 2) and is not empty.\n",
        "    *   If the data is missing or empty, it prints an error message and includes a commented-out `exit()` call, indicating that execution should ideally stop as subsequent steps would fail.\n",
        "\n",
        "2.  **Data Extraction (List Comprehensions):**\n",
        "    *   Uses list comprehensions for efficient extraction:\n",
        "        *   `texts = [item['text'] for item in all_spoiler_data]`: Creates a list `texts` containing only the string value associated with the `'text'` key from each dictionary in `all_spoiler_data`. This list holds the content that will be embedded.\n",
        "        *   `metadatas = [item['metadata'] for item in all_spoiler_data]`: Creates a list `metadatas` containing only the dictionary value associated with the `'metadata'` key from each item. These dictionaries will be stored alongside the vectors and can be used for filtering queries later.\n",
        "\n",
        "3.  **Unique ID Generation:**\n",
        "    *   Generates a unique string ID for each item in `all_spoiler_data`. Uniqueness is crucial for ChromaDB.\n",
        "    *   **Strategy:** Creates IDs by combining the season and episode number (extracted from the item's metadata) with a sequential index *within that specific episode*. This ensures IDs like `s01e01_0`, `s01e01_1`, `s01e02_0`, etc.\n",
        "    *   **Implementation:**\n",
        "        *   Initializes an empty list `ids` and a dictionary `event_counters` to store the next available index for each episode key (e.g., `\"s01e01\"`).\n",
        "        *   Iterates through `all_spoiler_data`.\n",
        "        *   Extracts `season` and `episode` from the `metadata`.\n",
        "        *   Formats an `episode_key` string (e.g., `s01e01`).\n",
        "        *   Uses the `event_counters` dictionary to get the current index for that episode (initializing to 0 if the key is new).\n",
        "        *   Constructs the ID string (e.g., `f\"{episode_key}_{current_index}\"`).\n",
        "        *   Appends the generated ID to the `ids` list.\n",
        "        *   Increments the counter for that `episode_key` in `event_counters`.\n",
        "\n",
        "4.  **Logging and Verification:**\n",
        "    *   Prints the number of items prepared in each list (`texts`, `metadatas`, `ids`).\n",
        "    *   Prints the first few examples (ID, metadata, truncated text) for visual verification of the prepared data structure.\n",
        "    *   **Crucial Validation:** Checks if the lengths of the three generated lists (`texts`, `metadatas`, `ids`) are all equal to each other *and* equal to the length of the original `all_spoiler_data`. This ensures data integrity and prevents errors when adding to ChromaDB. Prints an error if lengths mismatch.\n",
        "\n",
        "5.  **Error Handling:**\n",
        "    *   Wraps the core preparation logic in a `try...except` block.\n",
        "    *   Catches `KeyError` specifically, which would occur if an item in `all_spoiler_data` is missing the expected `'text'` or `'metadata'` keys (or `'season'`, `'episode'` within metadata). It prints an informative error suggesting checking the JSON structure.\n",
        "    *   Catches general `Exception` for any other unexpected errors during the preparation process.\n",
        "\n",
        "## Context & Importance\n",
        "\n",
        "This snippet is a critical data transformation step in the index creation pipeline.\n",
        "*   It converts the heterogeneous list of dictionaries (`all_spoiler_data`) into the homogeneous lists (`texts`, `metadatas`, `ids`) that ChromaDB's `add` or `upsert` methods expect.\n",
        "*   The quality and correctness of these three lists directly determine what gets stored in the vector database.\n",
        "*   Generating **unique and meaningful IDs** is vital for managing documents within ChromaDB.\n",
        "*   The **length validation check** is essential to catch errors *before* attempting to interact with ChromaDB, which requires these lists to be perfectly aligned. Failure in this snippet means the data cannot be correctly ingested into the database in the next step."
      ],
      "metadata": {
        "id": "UC9_27BgBaGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Snippet 4: Prepare Data for ChromaDB ===\n",
        "\n",
        "print(\"--- Running Snippet 4: Prepare Data for ChromaDB ---\")\n",
        "\n",
        "# Ensure all_spoiler_data exists from Snippet 2\n",
        "if 'all_spoiler_data' not in globals() or not all_spoiler_data:\n",
        "    print(\"Error: 'all_spoiler_data' not found or is empty. Please run Snippet 2 first.\")\n",
        "    # exit() # Optional: stop execution if data is missing\n",
        "else:\n",
        "    print(f\"Preparing {len(all_spoiler_data)} items for ChromaDB...\")\n",
        "\n",
        "    try:\n",
        "        # Extract text documents\n",
        "        texts = [item['text'] for item in all_spoiler_data]\n",
        "\n",
        "        # Extract metadata dictionaries\n",
        "        metadatas = [item['metadata'] for item in all_spoiler_data]\n",
        "\n",
        "        # Create unique IDs (e.g., s01e01_0, s01e01_1, s01e02_0, ...)\n",
        "        # Using episode and index within episode for uniqueness\n",
        "        ids = []\n",
        "        event_counters = {} # Track index within each episode\n",
        "        for item in all_spoiler_data:\n",
        "            season = item['metadata']['season']\n",
        "            episode = item['metadata']['episode']\n",
        "            episode_key = f\"s{season:02d}e{episode:02d}\"\n",
        "            if episode_key not in event_counters:\n",
        "                event_counters[episode_key] = 0\n",
        "            current_index = event_counters[episode_key]\n",
        "            ids.append(f\"{episode_key}_{current_index}\")\n",
        "            event_counters[episode_key] += 1\n",
        "\n",
        "        print(f\"Prepared {len(texts)} texts.\")\n",
        "        print(f\"Prepared {len(metadatas)} metadata entries.\")\n",
        "        print(f\"Generated {len(ids)} unique IDs.\")\n",
        "\n",
        "        # Optional: Print first few examples to verify\n",
        "        print(\"\\nExample Data:\")\n",
        "        for i in range(min(3, len(texts))):\n",
        "            print(f\"  ID: {ids[i]}, Metadata: {metadatas[i]}, Text: '{texts[i][:50]}...'\")\n",
        "\n",
        "        # Basic validation\n",
        "        if not (len(texts) == len(metadatas) == len(ids) == len(all_spoiler_data)):\n",
        "            print(\"Error: Length mismatch between texts, metadatas, and IDs!\")\n",
        "        else:\n",
        "            print(\"\\nData preparation successful.\")\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Missing key {e} in one of the spoiler data dictionaries.\")\n",
        "        print(\"Please check the structure of your JSON files.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during data preparation: {e}\")\n",
        "\n",
        "\n",
        "print(\"--- Finished Snippet 4 ---\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T08:14:31.954008Z",
          "iopub.execute_input": "2025-04-22T08:14:31.954281Z",
          "iopub.status.idle": "2025-04-22T08:14:31.963191Z",
          "shell.execute_reply.started": "2025-04-22T08:14:31.954262Z",
          "shell.execute_reply": "2025-04-22T08:14:31.962419Z"
        },
        "id": "A63HVjRVBaGi",
        "outputId": "9027eaad-41f3-4d0e-cc8c-1ff2f8423791"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "--- Running Snippet 4: Prepare Data for ChromaDB ---\nPreparing 466 items for ChromaDB...\nPrepared 466 texts.\nPrepared 466 metadata entries.\nGenerated 466 unique IDs.\n\nExample Data:\n  ID: s01e01_0, Metadata: {'season': 1, 'episode': 1}, Text: 'Homicide Detective Jimmy McNulty investigates the ...'\n  ID: s01e01_1, Metadata: {'season': 1, 'episode': 1}, Text: 'McNulty persuades an eyewitness to the Snot Boogie...'\n  ID: s01e01_2, Metadata: {'season': 1, 'episode': 1}, Text: 'During D'Angelo Barksdale's murder trial, key witn...'\n\nData preparation successful.\n--- Finished Snippet 4 ---\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Snippet 5: Initialize ChromaDB and Create Collection ---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This snippet initializes the connection to the ChromaDB vector database, ensuring it persists on disk, and then creates or retrieves the specific collection where the spoiler embeddings and metadata will be stored. This sets up the database structure ready to receive the data prepared in the previous steps.\n",
        "\n",
        "## Key Actions\n",
        "\n",
        "1.  **Import Libraries:** Imports `chromadb` for database interaction and `os` for filesystem operations (specifically, creating directories).\n",
        "2.  **Use Configuration:** References `OUTPUT_DB_PATH` (where the database files will be stored) and `COLLECTION_NAME` (the name of the \"table\" within the database) defined in Snippet 1.\n",
        "3.  **Ensure Output Directory:**\n",
        "    *   Uses `os.makedirs(OUTPUT_DB_PATH, exist_ok=True)` to create the directory specified by `OUTPUT_DB_PATH` if it doesn't already exist.\n",
        "    *   `exist_ok=True` prevents an error if the directory already exists. This is good practice before initializing a `PersistentClient` which expects the path to be available.\n",
        "4.  **Initialize ChromaDB Client:**\n",
        "    *   Uses a `try...except` block for robust initialization.\n",
        "    *   Creates a `PersistentClient` instance: `client = chromadb.PersistentClient(path=OUTPUT_DB_PATH)`.\n",
        "        *   `PersistentClient`: This type of client tells ChromaDB to store its data (vectors, metadata, index structures) on the local filesystem at the specified `path`. This allows the database to persist across script runs or notebook sessions, unlike an in-memory client.\n",
        "    *   Logs success or prints an error message if initialization fails (e.g., due to permissions issues). Includes an optional commented-out `exit()` for halting execution on failure.\n",
        "5.  **Get or Create Collection:**\n",
        "    *   Uses a `try...except` block for robustness.\n",
        "    *   Calls `collection = client.get_or_create_collection(...)`:\n",
        "        *   This is an idempotent operation: If a collection with the specified `name` already exists in the database at `OUTPUT_DB_PATH`, it retrieves and returns that collection object. If it doesn't exist, it creates a new collection with that name and returns it. This prevents errors if the script is run multiple times.\n",
        "        *   `name=COLLECTION_NAME`: Specifies the desired name for the collection.\n",
        "        *   `metadata={\"hnsw:space\": \"cosine\"}`: **Crucially**, this sets the distance metric used for similarity search within the collection's index (HNSW is the default index type). `cosine` distance (or cosine similarity) is generally recommended and performs well for sentence embeddings like those produced by `all-MiniLM-L6-v2`.\n",
        "    *   Logs success, indicating the collection is ready, and prints the current number of items in the collection (`collection.count()`). This count will be 0 if the collection was just created, or non-zero if the snippet is re-run after data has been added previously.\n",
        "    *   Catches and logs any errors during collection access/creation. Includes an optional commented-out `exit()`.\n",
        "\n",
        "## Context & Importance\n",
        "\n",
        "This snippet establishes the actual database environment where the vector data will live.\n",
        "*   **Persistence:** Using `PersistentClient` ensures that the created index is saved to disk and can be reloaded later or in different sessions without rebuilding it from scratch (as long as the `OUTPUT_DB_PATH` remains accessible).\n",
        "*   **Collection Setup:** `get_or_create_collection` provides robustness against re-running the script and correctly sets up the collection structure.\n",
        "*   **Distance Metric:** Specifying `\"hnsw:space\": \"cosine\"` aligns the database's similarity calculation with the nature of the sentence embeddings being used, ensuring meaningful search results later.\n",
        "*   This step is the **immediate prerequisite** for adding the prepared data (`texts`, `metadatas`, `ids` from Snippet 4) into the database using the `collection.add()` method in the next snippet. Failure here means there is no database collection ready to accept the data."
      ],
      "metadata": {
        "id": "QGBwQolNBaGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Snippet 5: Initialize ChromaDB and Create Collection ===\n",
        "\n",
        "import chromadb\n",
        "import os\n",
        "\n",
        "# Configuration from Snippet 1\n",
        "OUTPUT_DB_PATH = \"/kaggle/working/the_wire_s1_chroma_db_3\"\n",
        "COLLECTION_NAME = \"wire_s1_spoilers_3\"\n",
        "\n",
        "print(\"--- Running Snippet 5: Initialize ChromaDB ---\")\n",
        "\n",
        "# Ensure the output directory exists (PersistentClient usually creates it, but good practice)\n",
        "print(f\"Ensuring output directory exists: {OUTPUT_DB_PATH}\")\n",
        "os.makedirs(OUTPUT_DB_PATH, exist_ok=True)\n",
        "\n",
        "print(f\"Initializing ChromaDB PersistentClient at: {OUTPUT_DB_PATH}\")\n",
        "try:\n",
        "    client = chromadb.PersistentClient(path=OUTPUT_DB_PATH)\n",
        "    print(\"ChromaDB client initialized.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing ChromaDB client: {e}\")\n",
        "    # exit() # Optional\n",
        "\n",
        "print(f\"Getting or creating collection: '{COLLECTION_NAME}'\")\n",
        "try:\n",
        "    # Use get_or_create_collection for robustness\n",
        "    # Specify cosine distance as it's good for sentence embeddings\n",
        "    collection = client.get_or_create_collection(\n",
        "        name=COLLECTION_NAME,\n",
        "        metadata={\"hnsw:space\": \"cosine\"}\n",
        "    )\n",
        "    print(f\"Collection '{COLLECTION_NAME}' ready. Current item count: {collection.count()}\")\n",
        "    # Note: Count will be 0 if newly created, or >0 if re-running and collection persisted somehow\n",
        "except Exception as e:\n",
        "    print(f\"Error getting or creating ChromaDB collection: {e}\")\n",
        "    # exit() # Optional\n",
        "\n",
        "print(\"--- Finished Snippet 5 ---\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T08:14:48.691011Z",
          "iopub.execute_input": "2025-04-22T08:14:48.691530Z",
          "iopub.status.idle": "2025-04-22T08:14:48.968795Z",
          "shell.execute_reply.started": "2025-04-22T08:14:48.691498Z",
          "shell.execute_reply": "2025-04-22T08:14:48.968076Z"
        },
        "id": "BhwgDU8jBaGi",
        "outputId": "79ceb86f-7ba7-4e83-b3d4-e73862ffb103"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "--- Running Snippet 5: Initialize ChromaDB ---\nEnsuring output directory exists: /kaggle/working/the_wire_s1_chroma_db_3\nInitializing ChromaDB PersistentClient at: /kaggle/working/the_wire_s1_chroma_db_3\nChromaDB client initialized.\nGetting or creating collection: 'wire_s1_spoilers_3'\nCollection 'wire_s1_spoilers_3' ready. Current item count: 0\n--- Finished Snippet 5 ---\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Snippet 6: Embed and Add Data to ChromaDB ---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This snippet performs the core task of the index creation process: generating vector embeddings for the prepared text data using the loaded Sentence Transformer model and then adding the data (embeddings, original text, metadata, and unique IDs) into the designated ChromaDB collection in batches.\n",
        "\n",
        "## Key Actions\n",
        "\n",
        "1.  **Prerequisite Checks:**\n",
        "    *   Verifies that essential variables from previous snippets (`embedding_model`, `collection`, `ids`, `texts`, `metadatas`) exist in the global scope. If any are missing, it prints an error and halts execution (`exit()`), preventing errors in subsequent operations.\n",
        "2.  **Configuration & Initialization:**\n",
        "    *   Uses the `BATCH_SIZE` defined in Snippet 1 to control how many items are processed at once.\n",
        "    *   Prints status messages indicating the start of the process, the total number of items, and the batch size.\n",
        "    *   Initializes `start_time` using `time.time()` to measure the duration of the embedding and adding process.\n",
        "    *   Initializes `total_items_added` counter to track progress.\n",
        "3.  **Batch Processing Loop:**\n",
        "    *   Iterates through the data (`ids`, `texts`, `metadatas`) in steps of `BATCH_SIZE` using a `for` loop and `range(0, len(texts), BATCH_SIZE)`. Batching is crucial for managing memory (especially GPU VRAM during embedding) and preventing potential timeouts or crashes when dealing with large datasets.\n",
        "    *   **Data Slicing:** Inside the loop, it extracts slices of the `ids`, `texts`, and `metadatas` lists corresponding to the current batch using Python list slicing (e.g., `ids[i:i+BATCH_SIZE]`).\n",
        "    *   **Progress Logging:** Prints the current batch number and the range of item indices being processed in that batch.\n",
        "    *   **Embedding Generation:**\n",
        "        *   Uses a `try...except` block for robustness.\n",
        "        *   Calls `embedding_model.encode()` on the `batch_texts`.\n",
        "            *   `convert_to_numpy=True`: Initially gets the embeddings as a NumPy array for potential efficiency.\n",
        "            *   `show_progress_bar=False`: Suppresses the default progress bar from the `sentence-transformers` library to keep the console output cleaner within the loop.\n",
        "            *   `device='cuda'`: Explicitly ensures the computation happens on the GPU, leveraging the model loaded onto CUDA in Snippet 3.\n",
        "        *   Converts the resulting NumPy array (`batch_embeddings_np`) into a list of lists (`batch_embeddings_list`) using `.tolist()`, as ChromaDB's `add` method expects embeddings in this format.\n",
        "        *   Logs the number of embeddings generated for the batch.\n",
        "        *   If an error occurs during embedding, it prints an error message and uses `continue` to skip the rest of the loop for this batch and proceed to the next one.\n",
        "    *   **Adding Data to ChromaDB:**\n",
        "        *   Uses a `try...except` block.\n",
        "        *   Calls `collection.add()` to insert the current batch's data into the ChromaDB collection.\n",
        "            *   `ids=batch_ids`: Provides the unique identifiers for the documents.\n",
        "            *   `embeddings=batch_embeddings_list`: Provides the generated vector embeddings.\n",
        "            *   `metadatas=batch_metadatas`: Provides the associated metadata dictionaries.\n",
        "            *   `documents=batch_texts`: **Crucially**, also stores the original text content alongside the embedding. This allows retrieving the actual text during queries without needing a separate lookup based on ID.\n",
        "        *   Increments `total_items_added` by the number of items successfully processed in the batch.\n",
        "        *   Logs the successful addition of the batch.\n",
        "        *   If an error occurs while adding data (e.g., database connectivity issue, data validation error within ChromaDB), it prints an error message. (Note: Depending on the error, one might choose to stop the entire process or just log and continue).\n",
        "4.  **Timing and Summary:**\n",
        "    *   Calculates the total `elapsed_time` after the loop finishes.\n",
        "    *   Prints the total time taken and the `total_items_added`.\n",
        "5.  **Verification:**\n",
        "    *   Performs a final check by calling `collection.count()` to get the total number of items currently stored in the ChromaDB collection.\n",
        "    *   Compares this `final_count` to the expected number (`len(texts)`).\n",
        "    *   Prints a confirmation message if the counts match or a warning if they differ, indicating potential issues during the add process (e.g., skipped batches due to errors).\n",
        "6.  **Final Instructions:**\n",
        "    *   Prints the path (`OUTPUT_DB_PATH`) where the populated ChromaDB index data is stored.\n",
        "    *   Provides guidance on the next logical step in a typical Kaggle workflow: committing the notebook and saving the contents of the output directory as a new Kaggle Dataset so the created index can be easily reused in other notebooks.\n",
        "\n",
        "## Context & Importance\n",
        "\n",
        "This is the **most computationally intensive** part of the index creation pipeline.\n",
        "*   It translates the textual data into high-dimensional vectors using the ML model (embedding).\n",
        "*   It populates the vector database with these embeddings, along with their associated IDs, metadata, and the original text documents.\n",
        "*   The use of **batching** and **GPU acceleration** (`device='cuda'`) is critical for making this process feasible for non-trivial amounts of data within reasonable time and memory limits.\n",
        "*   Storing the **original documents** alongside the embeddings via `collection.add(documents=...)` is a key feature, simplifying later retrieval steps.\n",
        "*   The successful execution of this snippet results in a **fully populated, persistent ChromaDB vector index** stored at `OUTPUT_DB_PATH`, ready to be saved and used for semantic search and RAG applications."
      ],
      "metadata": {
        "id": "C4CE5rfHBaGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Snippet 6: Embed and Add Data to ChromaDB ===\n",
        "\n",
        "import time\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "# Ensure variables from previous snippets exist\n",
        "if 'embedding_model' not in globals(): print(\"Error: embedding_model not found.\"); exit()\n",
        "if 'collection' not in globals(): print(\"Error: collection not found.\"); exit()\n",
        "if 'ids' not in globals(): print(\"Error: ids not found.\"); exit()\n",
        "if 'texts' not in globals(): print(\"Error: texts not found.\"); exit()\n",
        "if 'metadatas' not in globals(): print(\"Error: metadatas not found.\"); exit()\n",
        "\n",
        "# Configuration from Snippet 1\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "print(\"--- Running Snippet 6: Embed and Add Data ---\")\n",
        "print(f\"Embedding {len(texts)} texts and adding to ChromaDB in batches of {BATCH_SIZE}...\")\n",
        "\n",
        "start_time = time.time()\n",
        "total_items_added = 0\n",
        "\n",
        "# Process data in batches\n",
        "for i in range(0, len(texts), BATCH_SIZE):\n",
        "    # Slice data for the current batch\n",
        "    batch_ids = ids[i:i+BATCH_SIZE]\n",
        "    batch_texts = texts[i:i+BATCH_SIZE]\n",
        "    batch_metadatas = metadatas[i:i+BATCH_SIZE]\n",
        "\n",
        "    current_batch_num = i // BATCH_SIZE + 1\n",
        "    total_batches = (len(texts) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "    print(f\"  Processing Batch {current_batch_num}/{total_batches} (Items {i+1} to {min(i+BATCH_SIZE, len(texts))})...\")\n",
        "\n",
        "    # Generate embeddings for the current batch\n",
        "    try:\n",
        "        batch_embeddings_np = embedding_model.encode(\n",
        "            batch_texts,\n",
        "            convert_to_numpy=True, # Get numpy array\n",
        "            show_progress_bar=False, # Keep output clean\n",
        "            device='cuda' # Ensure GPU is used\n",
        "        )\n",
        "        # Convert numpy array to list of lists for ChromaDB\n",
        "        batch_embeddings_list = batch_embeddings_np.tolist()\n",
        "        print(f\"    Generated {len(batch_embeddings_list)} embeddings for batch.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    Error generating embeddings for batch {current_batch_num}: {e}\")\n",
        "        print(\"    Skipping this batch.\")\n",
        "        continue # Skip to the next batch\n",
        "\n",
        "    # Add the batch to the ChromaDB collection\n",
        "    try:\n",
        "        collection.add(\n",
        "            ids=batch_ids,\n",
        "            embeddings=batch_embeddings_list,\n",
        "            metadatas=batch_metadatas,\n",
        "            documents=batch_texts # Store original text as well\n",
        "        )\n",
        "        total_items_added += len(batch_ids)\n",
        "        print(f\"    Successfully added batch {current_batch_num} to ChromaDB.\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Error adding batch {current_batch_num} to ChromaDB: {e}\")\n",
        "        # Consider whether to stop or continue if a batch fails\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(f\"\\nFinished embedding and adding data in {elapsed_time:.2f} seconds.\")\n",
        "print(f\"Total items added attempt: {total_items_added} (should match total items if no errors).\")\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"\\nVerifying collection count...\")\n",
        "try:\n",
        "    final_count = collection.count()\n",
        "    print(f\"Final item count in collection '{COLLECTION_NAME}': {final_count}\")\n",
        "    if final_count != len(texts):\n",
        "        print(f\"Warning: Expected {len(texts)} items, but final collection count is {final_count}.\")\n",
        "    else:\n",
        "        print(\"Collection count matches expected number of items.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error getting final collection count: {e}\")\n",
        "\n",
        "print(f\"\\nChromaDB index data should now be populated in: {OUTPUT_DB_PATH}\")\n",
        "print(\"Next step: Commit the notebook and save this output directory as a new Kaggle Dataset.\")\n",
        "print(\"--- Finished Snippet 6 ---\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T08:14:59.596604Z",
          "iopub.execute_input": "2025-04-22T08:14:59.597133Z",
          "iopub.status.idle": "2025-04-22T08:15:00.774809Z",
          "shell.execute_reply.started": "2025-04-22T08:14:59.597111Z",
          "shell.execute_reply": "2025-04-22T08:15:00.774076Z"
        },
        "id": "iiS3lk_rBaGj",
        "outputId": "b65a7ca2-0fbc-4e0a-b8ab-8ac4614b2d10"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "--- Running Snippet 6: Embed and Add Data ---\nEmbedding 466 texts and adding to ChromaDB in batches of 128...\n  Processing Batch 1/4 (Items 1 to 128)...\n    Generated 128 embeddings for batch.\n    Successfully added batch 1 to ChromaDB.\n  Processing Batch 2/4 (Items 129 to 256)...\n    Generated 128 embeddings for batch.\n    Successfully added batch 2 to ChromaDB.\n  Processing Batch 3/4 (Items 257 to 384)...\n    Generated 128 embeddings for batch.\n    Successfully added batch 3 to ChromaDB.\n  Processing Batch 4/4 (Items 385 to 466)...\n    Generated 82 embeddings for batch.\n    Successfully added batch 4 to ChromaDB.\n\nFinished embedding and adding data in 1.17 seconds.\nTotal items added attempt: 466 (should match total items if no errors).\n\nVerifying collection count...\nFinal item count in collection 'wire_s1_spoilers_3': 466\nCollection count matches expected number of items.\n\nChromaDB index data should now be populated in: /kaggle/working/the_wire_s1_chroma_db_3\nNext step: Commit the notebook and save this output directory as a new Kaggle Dataset.\n--- Finished Snippet 6 ---\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "EI9b78n8BaGj"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}