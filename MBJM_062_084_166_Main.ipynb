{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 11511387,
          "sourceType": "datasetVersion",
          "datasetId": 7218325
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MBJM\n",
        "\n",
        "## RAG-based Spoiler Detection and Context-Preserving Redaction\n",
        "\n",
        "\n",
        "**Names & SRNs of the team:**\n",
        "\n",
        "Hamsini V & PES1UG22AM062\n",
        "\n",
        "Kirti S & PES1UG22AM084\n",
        "\n",
        "Sudarshan Srinivasan & PES1UG22AM166"
      ],
      "metadata": {
        "id": "JSz0CRAxAlll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Snippet 1: Install Unsloth & Dependencies ---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This snippet focuses on setting up the necessary Python environment by installing the `unsloth` library and its core dependencies. Unsloth is a library designed to significantly speed up Large Language Model (LLM) fine-tuning and inference while reducing memory usage, often leveraging techniques like quantization and optimized kernels.\n",
        "\n",
        "## Key Actions\n",
        "\n",
        "1.  **Install Unsloth from GitHub:**\n",
        "    *   `!pip install \"unsloth[conda] @ git+https://github.com/unslothai/unsloth.git\" -q`\n",
        "        *   `!pip install`: Executes the pip package installer. The `!` prefix indicates this is likely run in an environment like a Jupyter Notebook or Google Colab where shell commands can be directly executed.\n",
        "        *   `\"unsloth[conda]\"`: Specifies the `unsloth` package. The `[conda]` extra suggests installing optional dependencies potentially optimized for or commonly used within Conda environments, although it installs via pip. This might pull specific versions of dependencies like PyTorch compatible with common Conda setups.\n",
        "        *   `@ git+https://github.com/unslothai/unsloth.git`: Instructs pip to install the package directly from the Unsloth AI GitHub repository. This ensures the latest development version is installed, which might contain newer features or bug fixes compared to the version on PyPI (the Python Package Index).\n",
        "        *   `-q`: The \"quiet\" flag, minimizing the installation output logged to the console.\n",
        "\n",
        "2.  **Install Core Dependencies:**\n",
        "    *   `!pip install transformers accelerate bitsandbytes sentence-transformers chromadb torch -q`\n",
        "        *   Installs several essential libraries for modern NLP and ML workflows:\n",
        "            *   `transformers`: Hugging Face's library for accessing pre-trained models (LLMs, embedding models, etc.) and related tools.\n",
        "            *   `accelerate`: Hugging Face's library for simplifying distributed training and inference across various hardware setups (multi-GPU, TPU). Unsloth often integrates with it.\n",
        "            *   `bitsandbytes`: Crucial library for quantization (e.g., loading models in 4-bit or 8-bit precision), which is a key technique used by Unsloth to reduce memory footprint. Required for loading `*-bnb-4bit` models.\n",
        "            *   `sentence-transformers`: A library built on `transformers` and `torch`, specifically designed for easy computation of dense vector embeddings (sentence embeddings). Used here for the RAG retrieval step.\n",
        "            *   `chromadb`: A client library for ChromaDB, an open-source vector database used to store and query embeddings efficiently.\n",
        "            *   `torch`: The PyTorch deep learning framework, the foundation for most of these libraries.\n",
        "        *   `-q`: Quiet installation.\n",
        "\n",
        "3.  **Verification Step:**\n",
        "    *   A `try...except` block attempts to import the newly installed libraries (`unsloth`, `transformers`, `accelerate`, `bitsandbytes`, `torch`, `sentence_transformers`, `chromadb`).\n",
        "    *   This confirms whether the installation was successful *within the current Python kernel's environment*. Pip installs packages, but importing verifies they are accessible to the running script/notebook.\n",
        "    *   Sets a boolean flag `INSTALL_SUCCESS` to `True` only if all imports succeed without error.\n",
        "    *   Prints success or error messages based on the import outcome.\n",
        "\n",
        "## Context & Importance\n",
        "\n",
        "This installation step is **critical** and must be executed successfully **before** any subsequent code that relies on these libraries (especially Unsloth's `FastLanguageModel`, Hugging Face models, embedding generation, or ChromaDB interactions). Installing directly from GitHub ensures access to the latest Unsloth optimizations. The inclusion of `bitsandbytes` strongly suggests that the intention is to load quantized models (like 4-bit models) for memory efficiency."
      ],
      "metadata": {
        "id": "l4E9J7_kAllo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Snippet 1 (Revised for Unsloth): Install Unsloth + Dependencies ===\n",
        "\n",
        "print(\"--- Running Snippet 1 (Revised for Unsloth): Install Unsloth ---\")\n",
        "print(\"Installing Unsloth, transformers, accelerate, bitsandbytes...\")\n",
        "\n",
        "# NOTE: Assuming pip commands are run in the environment (e.g., notebook cell)\n",
        "!pip install \"unsloth[conda] @ git+https://github.com/unslothai/unsloth.git\" -q # Using conda env as example\n",
        "!pip install transformers accelerate bitsandbytes sentence-transformers chromadb torch -q\n",
        "\n",
        "print(\"Installed Unsloth and other dependencies. (Assumed executed separately)\")\n",
        "\n",
        "# Verify installation attempt\n",
        "print(\"\\nVerifying installation...\")\n",
        "INSTALL_SUCCESS = False # Assume failure initially\n",
        "try:\n",
        "    import unsloth\n",
        "    import transformers\n",
        "    import accelerate\n",
        "    import bitsandbytes\n",
        "    import torch\n",
        "    import sentence_transformers\n",
        "    import chromadb\n",
        "    print(\"Successfully imported Unsloth and required libraries.\")\n",
        "    INSTALL_SUCCESS = True\n",
        "except ImportError as e:\n",
        "    print(f\"ERROR: Failed to import one of the required libraries: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during import check: {e}\")\n",
        "\n",
        "print(\"\\nRequired libraries installation attempted.\")\n",
        "print(\"--- Finished Snippet 1 (Revised for Unsloth) ---\")\n",
        "\n",
        "\n",
        "# ========================================================================\n",
        "# ========= Global Imports and Setup =====================================\n",
        "# ========================================================================\n",
        "import json\n",
        "import os\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import re # For sentence splitting later\n",
        "import time\n",
        "import shutil # Library for copying files/directories\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# === Configuration ===\n",
        "INPUT_INDEX_PATH = \"/kaggle/input/the-wire-s1-chroma-db-3/the_wire_s1_chroma_db_3\" # Replace with your actual path if different\n",
        "WRITABLE_INDEX_PATH = \"/kaggle/working/the_wire_s1_chroma_db_writable_2\"\n",
        "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'\n",
        "LLM_MODEL_ID = \"unsloth/gemma-2-9b-it-bnb-4bit\"\n",
        "COLLECTION_NAME = \"wire_s1_spoilers_3\" # Ensure this matches the collection name in your index\n",
        "\n",
        "# --- Flags ---\n",
        "EMBEDDING_LOAD_SUCCESS = False\n",
        "INDEX_LOAD_SUCCESS = False\n",
        "LLM_LOAD_SUCCESS = False\n",
        "# Assuming INSTALL_SUCCESS is handled by Snippet 1"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T11:33:47.783014Z",
          "iopub.execute_input": "2025-04-23T11:33:47.783238Z",
          "iopub.status.idle": "2025-04-23T11:34:17.356190Z",
          "shell.execute_reply.started": "2025-04-23T11:33:47.783217Z",
          "shell.execute_reply": "2025-04-23T11:34:17.355366Z"
        },
        "id": "cS9jLoiKAllp",
        "outputId": "ca2cc3a9-976b-4d80-fb8d-722a58dd582d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "--- Running Snippet 1 (Revised for Unsloth): Install Unsloth ---\nInstalling Unsloth, transformers, accelerate, bitsandbytes...\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nopentelemetry-proto 1.32.1 requires protobuf<6.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\ntensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nunsloth-zoo 2025.3.17 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mInstalled Unsloth and other dependencies. (Assumed executed separately)\n\nVerifying installation...\n🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "2025-04-23 11:34:08.938861: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745408048.961071     223 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745408048.967853     223 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n🦥 Unsloth Zoo will now patch everything to make training faster!\nSuccessfully imported Unsloth and required libraries.\n\nRequired libraries installation attempted.\n--- Finished Snippet 1 (Revised for Unsloth) ---\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Snippet 2 (Revised): Copy Index and Load Components ---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This revised snippet is responsible for setting up the \"retrieval\" part of a potential RAG (Retrieval-Augmented Generation) system. It performs three critical actions:\n",
        "1.  Loads the sentence embedding model required to understand query text and compare it to stored documents.\n",
        "2.  Copies a pre-built ChromaDB vector index from a read-only input location to a writable working directory. This is often necessary in environments like Kaggle or Docker containers where input data is immutable.\n",
        "3.  Loads the ChromaDB client and accesses the specific collection containing the document embeddings from the newly copied writable location.\n",
        "\n",
        "## Key Actions\n",
        "\n",
        "1.  **Configuration Display:**\n",
        "    *   Prints the values of the configuration variables (`INPUT_INDEX_PATH`, `WRITABLE_INDEX_PATH`, `EMBEDDING_MODEL_NAME`, `LLM_MODEL_ID`, `COLLECTION_NAME`) defined previously. This serves as a confirmation and aids debugging.\n",
        "\n",
        "2.  **Load Embedding Model:**\n",
        "    *   Attempts to load the sentence embedding model specified by `EMBEDDING_MODEL_NAME` (`'all-MiniLM-L6-v2'`) using the `SentenceTransformer` class.\n",
        "    *   **Device Selection:** Automatically detects if a CUDA-enabled GPU is available (`torch.cuda.is_available()`) and sets the target device (`'cuda'` or `'cpu'`) accordingly. Loading the model onto a GPU significantly speeds up embedding computation.\n",
        "    *   **Instantiation:** `embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)` loads the pre-trained model weights onto the selected device.\n",
        "    *   **Status Update:** Sets the `EMBEDDING_LOAD_SUCCESS` flag to `True` upon successful loading, or `False` if any exception occurs during the process. Error messages are printed if loading fails.\n",
        "\n",
        "3.  **Copy ChromaDB Index (Conditional):**\n",
        "    *   This entire block executes only if the embedding model was loaded successfully (`if EMBEDDING_LOAD_SUCCESS:`). While copying might technically be independent, accessing the collection later often implicitly relies on the correct embedding context.\n",
        "    *   **Source Path Validation:** Checks if the `INPUT_INDEX_PATH` exists using `os.path.exists()`. It includes a basic check to see if the path refers to a directory, raising a `FileNotFoundError` if the source index directory cannot be found.\n",
        "    *   **Destination Path Handling:**\n",
        "        *   Checks if the `WRITABLE_INDEX_PATH` already exists.\n",
        "        *   If it exists, `shutil.rmtree(WRITABLE_INDEX_PATH)` is called to **remove the existing directory and its contents**. This ensures a clean copy and prevents errors from `shutil.copytree` if the destination already exists. **Caution:** This deletes data in the target path.\n",
        "    *   **Copy Operation:** `shutil.copytree(INPUT_INDEX_PATH, WRITABLE_INDEX_PATH)` recursively copies the entire directory structure containing the ChromaDB index files from the source (read-only) path to the destination (writable) path.\n",
        "    *   **Error Handling:** Catches `FileNotFoundError` specifically if the source is missing and general `Exception` for other potential errors during the copy process (e.g., permissions issues, disk space). Sets `INDEX_LOAD_SUCCESS` to `False` if copying fails.\n",
        "\n",
        "4.  **Load ChromaDB Client and Collection:**\n",
        "    *   Performed *after* the index is successfully copied, within the same `try` block.\n",
        "    *   **Client Instantiation:** `client = chromadb.PersistentClient(path=WRITABLE_INDEX_PATH)` creates a ChromaDB client instance. `PersistentClient` connects to a database stored on the local filesystem at the specified path (the writable location).\n",
        "    *   **List Collections:** `client.list_collections()` retrieves metadata about all collections present in the database at the specified path. The names are printed for verification.\n",
        "    *   **Collection Verification:** Checks if a collection with the name specified in `COLLECTION_NAME` (`\"wire_s1_spoilers_3\"`) exists within the loaded database.\n",
        "    *   **Get Collection:** If the collection exists, `collection = client.get_collection(name=COLLECTION_NAME)` loads the specific collection object. Note: This assumes the collection metadata in the persistent storage includes the necessary embedding function information, or that ChromaDB can infer it; the embedding model loaded earlier (`embedding_model`) is used later for *querying*, not explicitly passed during collection loading here.\n",
        "    *   **Confirmation & Status Update:** Prints the number of items (`collection.count()`) in the loaded collection and sets `INDEX_LOAD_SUCCESS` to `True`.\n",
        "    *   **Error Handling:** Catches `ValueError` if the specified `COLLECTION_NAME` is not found and general `Exception` for other potential database loading issues. Sets `INDEX_LOAD_SUCCESS` to `False` on failure.\n",
        "\n",
        "5.  **Skipping Logic:**\n",
        "    *   If `EMBEDDING_LOAD_SUCCESS` is `False` (the embedding model failed to load), the entire index copying and loading process is skipped, and a message is printed.\n",
        "\n",
        "## Context & Importance\n",
        "\n",
        "This snippet is crucial for preparing the knowledge base for the RAG system.\n",
        "*   It ensures the **embedding model** (which translates text to vectors) is ready.\n",
        "*   It handles the practical requirement of making the **vector database** accessible in a writable location, a common step in restricted environments.\n",
        "*   It loads the actual **vector store collection** (`collection`) which will be queried later using the embedding model to find relevant context.\n",
        "*   The `EMBEDDING_LOAD_SUCCESS` and `INDEX_LOAD_SUCCESS` flags are vital checkpoints, ensuring these components are ready before proceeding to use them for retrieval or generation, preventing runtime errors later. Failure in this snippet likely means the RAG system cannot function correctly."
      ],
      "metadata": {
        "id": "dyulgX3vAllr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Snippet 2 (Revised): Copy Index and Load ===\n",
        "print(\"\\n--- Running Revised Snippet 2: Copy Index and Load ---\")\n",
        "print(\"--- Configuration ---\")\n",
        "print(f\"Input Index Path (Read-Only): {INPUT_INDEX_PATH}\")\n",
        "print(f\"Writable Index Path: {WRITABLE_INDEX_PATH}\")\n",
        "print(f\"Embedding Model Name: {EMBEDDING_MODEL_NAME}\")\n",
        "print(f\"LLM Model ID: {LLM_MODEL_ID}\")\n",
        "print(f\"Collection Name: {COLLECTION_NAME}\")\n",
        "\n",
        "print(\"\\nLoading embedding model...\")\n",
        "embedding_model = None\n",
        "try:\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device} for embedding model\")\n",
        "    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)\n",
        "    print(\"Embedding model loaded successfully.\")\n",
        "    EMBEDDING_LOAD_SUCCESS = True\n",
        "except Exception as e:\n",
        "    print(f\"Error loading embedding model: {e}\")\n",
        "    EMBEDDING_LOAD_SUCCESS = False\n",
        "\n",
        "print(f\"\\nCopying index from {INPUT_INDEX_PATH} to {WRITABLE_INDEX_PATH}...\")\n",
        "client = None\n",
        "collection = None\n",
        "if EMBEDDING_LOAD_SUCCESS:\n",
        "    try:\n",
        "        if not os.path.exists(INPUT_INDEX_PATH):\n",
        "            # Check if the INPUT path itself is the directory containing the index files\n",
        "            parent_dir = os.path.dirname(INPUT_INDEX_PATH)\n",
        "            base_name = os.path.basename(INPUT_INDEX_PATH)\n",
        "            # A simple check: does the parent exist and contain the base name?\n",
        "            # This might need refinement depending on exact Kaggle input structure\n",
        "            if os.path.isdir(INPUT_INDEX_PATH):\n",
        "                 print(f\" Source path seems valid: {INPUT_INDEX_PATH}\")\n",
        "            else:\n",
        "                 raise FileNotFoundError(f\"ChromaDB index source path not found or invalid: {INPUT_INDEX_PATH}.\")\n",
        "\n",
        "\n",
        "        if os.path.exists(WRITABLE_INDEX_PATH):\n",
        "            print(f\" Removing existing writable directory: {WRITABLE_INDEX_PATH}\")\n",
        "            shutil.rmtree(WRITABLE_INDEX_PATH)\n",
        "        shutil.copytree(INPUT_INDEX_PATH, WRITABLE_INDEX_PATH)\n",
        "        print(f\" Successfully copied index to writable location.\")\n",
        "\n",
        "        print(f\"\\nLoading ChromaDB index from writable path: {WRITABLE_INDEX_PATH}\")\n",
        "        client = chromadb.PersistentClient(path=WRITABLE_INDEX_PATH)\n",
        "        list_collections = client.list_collections()\n",
        "        print(f\"Available collections: {[c.name for c in list_collections]}\")\n",
        "        if any(c.name == COLLECTION_NAME for c in list_collections):\n",
        "            collection = client.get_collection(name=COLLECTION_NAME) # Make sure embedding_function isn't needed here if already loaded\n",
        "            print(f\"ChromaDB collection '{COLLECTION_NAME}' loaded successfully with {collection.count()} items.\")\n",
        "            INDEX_LOAD_SUCCESS = True\n",
        "        else:\n",
        "            raise ValueError(f\"Collection '{COLLECTION_NAME}' not found in the database at {WRITABLE_INDEX_PATH}.\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error during index copy/load: {e}\")\n",
        "        INDEX_LOAD_SUCCESS = False\n",
        "    except ValueError as e:\n",
        "        print(f\"Error loading collection: {e}\")\n",
        "        INDEX_LOAD_SUCCESS = False\n",
        "    except Exception as e:\n",
        "        print(f\"Error copying or loading ChromaDB index: {e}\")\n",
        "        INDEX_LOAD_SUCCESS = False\n",
        "else:\n",
        "    print(\"Skipping index copy/load because embedding model failed to load.\")\n",
        "print(\"\\n--- Finished Revised Snippet 2 ---\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T11:34:17.357173Z",
          "iopub.execute_input": "2025-04-23T11:34:17.357399Z",
          "iopub.status.idle": "2025-04-23T11:34:18.516134Z",
          "shell.execute_reply.started": "2025-04-23T11:34:17.357378Z",
          "shell.execute_reply": "2025-04-23T11:34:18.515310Z"
        },
        "id": "gurgsfl7Allr",
        "outputId": "d26791da-fa22-4ae8-fa3d-914503b4a134"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n--- Running Revised Snippet 2: Copy Index and Load ---\n--- Configuration ---\nInput Index Path (Read-Only): /kaggle/input/the-wire-s1-chroma-db-3/the_wire_s1_chroma_db_3\nWritable Index Path: /kaggle/working/the_wire_s1_chroma_db_writable_2\nEmbedding Model Name: all-MiniLM-L6-v2\nLLM Model ID: unsloth/gemma-2-9b-it-bnb-4bit\nCollection Name: wire_s1_spoilers_3\n\nLoading embedding model...\nUsing device: cuda for embedding model\nEmbedding model loaded successfully.\n\nCopying index from /kaggle/input/the-wire-s1-chroma-db-3/the_wire_s1_chroma_db_3 to /kaggle/working/the_wire_s1_chroma_db_writable_2...\n Removing existing writable directory: /kaggle/working/the_wire_s1_chroma_db_writable_2\n Successfully copied index to writable location.\n\nLoading ChromaDB index from writable path: /kaggle/working/the_wire_s1_chroma_db_writable_2\nAvailable collections: ['wire_s1_spoilers_3']\nChromaDB collection 'wire_s1_spoilers_3' loaded successfully with 466 items.\n\n--- Finished Revised Snippet 2 ---\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Snippet 3 (Revised): Helper Functions ---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This snippet defines essential helper functions required for the core logic of the application, specifically for parsing user progress and implementing the Retrieval-Augmented Generation (RAG) process with built-in spoiler filtering.\n",
        "\n",
        "## Key Actions\n",
        "\n",
        "1.  **Dependency Check:**\n",
        "    *   It first checks the status flags `EMBEDDING_LOAD_SUCCESS` and `INDEX_LOAD_SUCCESS` (set in previous snippets).\n",
        "    *   If either the embedding model or the ChromaDB index failed to load, it prints a warning, indicating that the RAG function defined later might fail or return incorrect results. This check promotes awareness of potential issues downstream.\n",
        "\n",
        "2.  **Define `parse_progress` Function:**\n",
        "    *   This function is designed to interpret user input representing their viewing progress in a series (e.g., \"S1E5\" for Season 1, Episode 5).\n",
        "    *   It uses regular expressions (`re.match`) to extract season and episode numbers reliably, ignoring case.\n",
        "    *   Includes validation to ensure the input is a string and that the extracted parts can be converted to integers.\n",
        "    *   Returns a tuple `(season, episode)` as integers if successful, otherwise returns `(None, None)` and prints a warning.\n",
        "\n",
        "3.  **Define `retrieve_and_filter_context` Function:**\n",
        "    *   This is the core function performing the retrieval and spoiler filtering logic.\n",
        "    *   It takes user input text, their current viewing progress (season/episode), and optional parameters for the number of results (`n_results`) and a similarity threshold (`distance_threshold`).\n",
        "    *   It retrieves relevant documents from the ChromaDB `collection` based on the semantic similarity of the `input_text`.\n",
        "    *   Crucially, it then filters these retrieved documents based on the user's progress, separating them into context that is safe to show (non-spoilers) and context that pertains to future episodes (potential spoilers).\n",
        "\n",
        "## Function Details\n",
        "\n",
        "### Function: `parse_progress(progress_str)`\n",
        "\n",
        "*   **Purpose:** Converts a string like 'S01E05' or 's2e10' into numerical season and episode.\n",
        "*   **Parameters:**\n",
        "    *   `progress_str`: The input string to parse.\n",
        "*   **Logic:**\n",
        "    1.  **Type Check:** Verifies `progress_str` is actually a string.\n",
        "    2.  **Regex Match:** Uses `re.match(r\"S(\\d+)E(\\d+)\", ..., re.IGNORECASE)` to find the pattern \"S<digits>E<digits>\" at the start of the string, ignoring case. `(\\d+)` captures one or more digits into groups.\n",
        "    3.  **Extraction & Conversion:** If a match is found, it attempts to convert the captured groups (season and episode numbers) into integers using `int()`.\n",
        "    4.  **Error Handling:** Returns `(None, None)` and prints warnings if the input is not a string, the regex pattern doesn't match, or the captured groups cannot be converted to integers.\n",
        "*   **Returns:** A tuple `(int, int)` representing `(season, episode)`, or `(None, None)` on failure.\n",
        "\n",
        "### Function: `retrieve_and_filter_context(input_text, user_season, user_episode, n_results=5, distance_threshold=0.5)`\n",
        "\n",
        "*   **Purpose:** Implements the spoiler-aware RAG retrieval process.\n",
        "*   **Parameters:**\n",
        "    *   `input_text` (str): The user's query or text for which context is needed.\n",
        "    *   `user_season` (int): The season number the user has watched up to.\n",
        "    *   `user_episode` (int): The episode number within `user_season` the user has watched up to.\n",
        "    *   `n_results` (int, optional, default=5): The number of top matching documents to retrieve from ChromaDB *for each sentence* in the input text.\n",
        "    *   `distance_threshold` (float, optional, default=0.5): The maximum semantic distance allowed for a retrieved document to be considered relevant (lower values mean higher similarity). Documents with distance greater than this are discarded.\n",
        "*   **Dependencies:**\n",
        "    *   Uses the global `embedding_model` (SentenceTransformer) and `collection` (ChromaDB) objects loaded in previous steps.\n",
        "    *   Checks `EMBEDDING_LOAD_SUCCESS` and `INDEX_LOAD_SUCCESS` flags internally for robustness.\n",
        "*   **Logic:**\n",
        "    1.  **Pre-Checks:** Verifies that the embedding model and index loaded successfully and that the respective objects (`embedding_model`, `collection`) are not `None`. Returns empty lists `([], [])` if dependencies are missing.\n",
        "    2.  **Sentence Splitting:** Breaks the `input_text` into individual sentences using `re.split`. This allows querying the vector database with more granular pieces of text, potentially retrieving more diverse and relevant context compared to embedding the entire input at once. Filters out very short or empty sentences.\n",
        "    3.  **Query Embedding:** Encodes the split sentences into vector embeddings using `embedding_model.encode()`. Handles potential errors during embedding.\n",
        "    4.  **ChromaDB Query:** Performs a batch query against the `collection` using the generated `query_embeddings`. It requests `n_results` matches for each sentence embedding, including document content (`documents`), metadata (`metadatas`), and similarity scores (`distances`). Handles potential query errors.\n",
        "    5.  **Result Processing & Filtering:**\n",
        "        *   Iterates through the results returned for each input sentence.\n",
        "        *   Iterates through the individual documents retrieved for that sentence.\n",
        "        *   **De-duplication:** Uses a `processed_ids` set to ensure each unique document from the database is processed only once, even if retrieved for multiple input sentences.\n",
        "        *   **Distance Filtering:** Skips documents whose `distance` exceeds the `distance_threshold`.\n",
        "        *   **Metadata Extraction:** Safely extracts `season` and `episode` numbers from the document's metadata, handling potential missing keys or non-integer values. Skips documents with invalid or missing metadata.\n",
        "        *   **Spoiler Classification:** Compares the document's season/episode (`spoiler_season`, `spoiler_episode`) to the user's progress (`user_season`, `user_episode`). A document is classified as a potential spoiler if its season is greater than the user's, OR if the season is the same but the episode is greater.\n",
        "        *   **Categorization:** Adds the document text (prefixed with its \"(S E)\" identifier for clarity) to one of two sets: `relevant_non_spoilers` or `identified_spoilers`. Using sets helps avoid duplicate text entries within each category.\n",
        "    6.  **Error Handling:** Includes checks for empty/malformed ChromaDB results and safe access to list indices within the nested loops.\n",
        "*   **Returns:** A tuple containing two lists: `(list(relevant_non_spoilers), list(identified_spoilers))`.\n",
        "\n",
        "## Context & Importance\n",
        "\n",
        "These helper functions are the building blocks for the application's core RAG functionality.\n",
        "*   `parse_progress` provides essential input validation and standardization for user viewing progress.\n",
        "*   `retrieve_and_filter_context` encapsulates the complex logic of:\n",
        "    *   Performing semantic search using vector embeddings (the \"Retrieval\" part).\n",
        "    *   Applying domain-specific rules (spoiler filtering based on season/episode metadata) to the retrieved results.\n",
        "    *   Preparing the context (separated into non-spoilers and spoilers) that will likely be fed into a Large Language Model (LLM) in a subsequent step (the \"Augmented Generation\" part).\n",
        "\n",
        "The robustness checks (dependency flags, error handling within functions) are critical for ensuring the application can handle potential issues like failed model loading or malformed data gracefully."
      ],
      "metadata": {
        "id": "ZpzK5eWYAllr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Snippet 3 (Revised): Helper Functions ===\n",
        "print(\"\\n--- Running Snippet 3: Defining Helper Functions ---\")\n",
        "\n",
        "# Ensure necessary variables/objects from previous steps exist\n",
        "if not (EMBEDDING_LOAD_SUCCESS and INDEX_LOAD_SUCCESS):\n",
        "    print(\"Warning: Embedding model or index did not load successfully. RAG function might fail.\")\n",
        "    # We'll add checks within the functions instead for more graceful failure\n",
        "\n",
        "# --- Helper Function to Parse Progress ---\n",
        "def parse_progress(progress_str):\n",
        "    \"\"\"\n",
        "    Parses a progress string like 'S1E5' or 's01e10' into season and episode integers.\n",
        "    Returns (season, episode) or (None, None) if parsing fails.\n",
        "    \"\"\"\n",
        "    if not isinstance(progress_str, str):\n",
        "        print(f\"Warning: Progress input is not a string: {progress_str}\")\n",
        "        return None, None\n",
        "    match = re.match(r\"S(\\d+)E(\\d+)\", progress_str, re.IGNORECASE)\n",
        "    if match:\n",
        "        try:\n",
        "            season = int(match.group(1))\n",
        "            episode = int(match.group(2))\n",
        "            return season, episode\n",
        "        except ValueError:\n",
        "            print(f\"Warning: Could not parse numbers in progress string '{progress_str}'.\")\n",
        "            return None, None\n",
        "    else:\n",
        "        print(f\"Warning: Could not parse progress string format '{progress_str}'. Expected S<num>E<num>.\")\n",
        "        return None, None\n",
        "print(\"Defined parse_progress function.\")\n",
        "\n",
        "\n",
        "# --- RAG + Filtering Function (Revised Name and Output) ---\n",
        "def retrieve_and_filter_context(input_text, user_season, user_episode, n_results=5, distance_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Retrieves relevant events from ChromaDB for the input_text, filters them based on user progress,\n",
        "    and returns two lists:\n",
        "    1. relevant_non_spoilers: Events at or before user's progress (safe context).\n",
        "    2. identified_spoilers: Events after user's progress (potential spoilers).\n",
        "    \"\"\"\n",
        "    global embedding_model, collection # Make sure globals are accessible\n",
        "    if not EMBEDDING_LOAD_SUCCESS or not INDEX_LOAD_SUCCESS:\n",
        "        print(\"Error in retrieve_and_filter_context: Embedding model or index not loaded.\")\n",
        "        return [], [] # Return empty lists\n",
        "    if embedding_model is None or collection is None:\n",
        "        print(\"Error in retrieve_and_filter_context: embedding_model or collection object is None.\")\n",
        "        return [], []\n",
        "\n",
        "    # print(f\"\\n--- Retrieving context for user S{user_season}E{user_episode} ---\")\n",
        "    # print(f\"Input Text (first 100 chars): {input_text[:100]}...\")\n",
        "\n",
        "    # Simple sentence splitting (can be improved)\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', input_text)\n",
        "    sentences = [s.strip() for s in sentences if s and len(s.strip()) > 5]\n",
        "    if not sentences:\n",
        "        # print(\"No meaningful sentences found in input text for RAG.\")\n",
        "        return [], []\n",
        "\n",
        "    # print(f\"Split into {len(sentences)} sentences for querying.\")\n",
        "    try:\n",
        "        query_embeddings = embedding_model.encode(sentences, convert_to_numpy=True, show_progress_bar=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during sentence embedding for RAG: {e}\")\n",
        "        return [], []\n",
        "\n",
        "    try:\n",
        "        results = collection.query(\n",
        "            query_embeddings=query_embeddings.tolist(),\n",
        "            n_results=n_results,\n",
        "            include=['metadatas', 'documents', 'distances']\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying ChromaDB: {e}\")\n",
        "        return [], []\n",
        "\n",
        "    relevant_non_spoilers = set()\n",
        "    identified_spoilers = set()\n",
        "    processed_ids = set() # Keep track of processed document IDs to avoid duplicates\n",
        "\n",
        "    if not results or not results.get('ids'):\n",
        "        # print(\"Warning: ChromaDB query returned no results.\")\n",
        "        return [], []\n",
        "\n",
        "    for i in range(len(sentences)): # Iterate through each sentence query's results\n",
        "        # Check if results are valid for this sentence\n",
        "        ids_list = results.get('ids', [])[i] if results.get('ids') and i < len(results['ids']) else None\n",
        "        metadatas_list = results.get('metadatas', [])[i] if results.get('metadatas') and i < len(results['metadatas']) else None\n",
        "        documents_list = results.get('documents', [])[i] if results.get('documents') and i < len(results['documents']) else None\n",
        "        distances_list = results.get('distances', [])[i] if results.get('distances') and i < len(results['distances']) else None\n",
        "\n",
        "        if not (ids_list and metadatas_list and documents_list and distances_list):\n",
        "             # print(f\"Warning: Inconsistent or missing results structure for sentence index {i}. Skipping.\")\n",
        "             continue\n",
        "\n",
        "        # Iterate through results for this specific sentence query\n",
        "        for j in range(len(ids_list)):\n",
        "            doc_id = ids_list[j]\n",
        "            # Skip if we've already processed this document ID from another sentence query\n",
        "            if doc_id in processed_ids:\n",
        "                continue\n",
        "\n",
        "            # Check index bounds for safety\n",
        "            if j < len(metadatas_list) and j < len(documents_list) and j < len(distances_list):\n",
        "                doc_meta = metadatas_list[j] if metadatas_list[j] is not None else {}\n",
        "                doc_text = documents_list[j] if documents_list[j] is not None else \"N/A\"\n",
        "                distance = distances_list[j] if distances_list[j] is not None else 1.0\n",
        "\n",
        "                # Filter by distance threshold\n",
        "                if distance > distance_threshold:\n",
        "                    continue\n",
        "\n",
        "                # Extract and validate metadata\n",
        "                spoiler_season_raw = doc_meta.get('season')\n",
        "                spoiler_episode_raw = doc_meta.get('episode')\n",
        "                spoiler_season, spoiler_episode = None, None\n",
        "                try:\n",
        "                    if spoiler_season_raw is not None: spoiler_season = int(spoiler_season_raw)\n",
        "                    if spoiler_episode_raw is not None: spoiler_episode = int(spoiler_episode_raw)\n",
        "                except (ValueError, TypeError):\n",
        "                    # print(f\"Warning: Invalid S/E format in metadata ID {doc_id}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                if spoiler_season is None or spoiler_episode is None:\n",
        "                    # print(f\"Warning: Missing or invalid metadata S/E in ID {doc_id}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Classify as spoiler or non-spoiler context based on user progress\n",
        "                is_potential_spoiler = False\n",
        "                if spoiler_season > user_season:\n",
        "                    is_potential_spoiler = True\n",
        "                elif spoiler_season == user_season and spoiler_episode > user_episode:\n",
        "                    is_potential_spoiler = True\n",
        "\n",
        "                # Add to the appropriate set, including S/E info for context\n",
        "                if is_potential_spoiler:\n",
        "                    # print(f\" Potential Spoiler Found (S{spoiler_season}E{spoiler_episode}, Dist: {distance:.2f}): {doc_text[:80]}...\")\n",
        "                    identified_spoilers.add(f\"(S{spoiler_season}E{spoiler_episode}) {doc_text}\") # Add context for LLM\n",
        "                else:\n",
        "                    # print(f\" Relevant Non-Spoiler Found (S{spoiler_season}E{spoiler_episode}, Dist: {distance:.2f}): {doc_text[:80]}...\")\n",
        "                    relevant_non_spoilers.add(f\"(S{spoiler_season}E{spoiler_episode}) {doc_text}\") # Add context for LLM\n",
        "\n",
        "                processed_ids.add(doc_id) # Mark this document ID as processed\n",
        "            else:\n",
        "                # print(f\"Warning: Index out of bounds for inner loop (j={j}) at outer loop index i={i}. Skipping result.\")\n",
        "                 pass # Should not happen if initial list checks pass, but safe to ignore\n",
        "\n",
        "\n",
        "    # print(f\"Retrieved {len(relevant_non_spoilers)} relevant non-spoilers and {len(identified_spoilers)} potential spoilers.\")\n",
        "    return list(relevant_non_spoilers), list(identified_spoilers)\n",
        "\n",
        "print(\"Defined retrieve_and_filter_context function.\")\n",
        "print(\"--- Finished Snippet 3 ---\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T11:34:18.518222Z",
          "iopub.execute_input": "2025-04-23T11:34:18.518448Z",
          "iopub.status.idle": "2025-04-23T11:34:18.534288Z",
          "shell.execute_reply.started": "2025-04-23T11:34:18.518430Z",
          "shell.execute_reply": "2025-04-23T11:34:18.533529Z"
        },
        "id": "StidObPMAlls",
        "outputId": "d233e38a-878d-4d30-a20b-3eb67985b7b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n--- Running Snippet 3: Defining Helper Functions ---\nDefined parse_progress function.\nDefined retrieve_and_filter_context function.\n--- Finished Snippet 3 ---\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Snippet 4 (Revised for Unsloth): Load LLM and Define Generation Function ---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This snippet is responsible for loading the specified Large Language Model (LLM) using the Unsloth library, which is optimized for speed and memory efficiency. It also defines a reusable function for generating text using the loaded model and tokenizer, incorporating standard generation parameters and error handling.\n",
        "\n",
        "## Key Actions\n",
        "\n",
        "1.  **Prerequisite Checks:**\n",
        "    *   Verifies the `INSTALL_SUCCESS` flag (from Snippet 1) to ensure necessary libraries like `unsloth`, `transformers`, `torch`, and `bitsandbytes` were installed correctly. If not, it prints an error and conceptually halts (though the `exit()` is commented out).\n",
        "    *   Verifies the `EMBEDDING_LOAD_SUCCESS` and `INDEX_LOAD_SUCCESS` flags (from Snippet 2 Revised) to ensure the retrieval components (embedding model and vector index) are ready. While not strictly required *just* to load the LLM, it implies the overall application requires these, so loading the LLM might be futile if they failed. It prints an error and conceptually halts if these checks fail.\n",
        "\n",
        "2.  **Load LLM with Unsloth:**\n",
        "    *   **Model Identifier:** Specifies the model to load using `LLM_MODEL_ID` (`\"unsloth/gemma-2-9b-it-bnb-4bit\"`). This is a 9 billion parameter Gemma 2 instruction-tuned model, quantized to 4-bit precision (`bnb-4bit`) and potentially optimized by Unsloth.\n",
        "    *   **Unsloth `FastLanguageModel`:** Uses `FastLanguageModel.from_pretrained()` provided by the `unsloth` library. This class is designed to load models with optimizations (like Flash Attention, optimized kernels) and quantization applied for faster inference and reduced VRAM usage compared to standard Hugging Face loading.\n",
        "    *   **Loading Parameters:**\n",
        "        *   `max_seq_length=4096`: Sets the maximum sequence length the model can handle. This impacts VRAM usage and the amount of context the model can process.\n",
        "        *   `dtype=None`: Allows Unsloth to automatically determine the optimal data type (like `bfloat16` on newer GPUs or `float16`) for computation, balancing speed and precision.\n",
        "        *   `load_in_4bit=True`: Explicitly instructs the loader to use 4-bit quantization via `bitsandbytes`. This is crucial for fitting large models like Gemma 9B into typical GPU VRAM limits.\n",
        "        *   `# token = \"hf_...\"`: A placeholder comment indicating where a Hugging Face access token would be added if required (e.g., for gated models).\n",
        "    *   **Output:** Returns the optimized `model` object and the corresponding `tokenizer`.\n",
        "    *   **Status Update:** Sets the `LLM_LOAD_SUCCESS` flag to `True` on success, `False` on failure.\n",
        "    *   **Error Handling:** Includes a `try...except` block to catch potential errors during model loading (e.g., download issues, VRAM exhaustion, configuration errors) and prints informative error messages.\n",
        "\n",
        "3.  **Define Unified Generation Function (`generate_text_unsloth`):**\n",
        "    *   **Purpose:** Creates a standardized function to interact with the loaded Unsloth model for text generation.\n",
        "    *   **Parameters:**\n",
        "        *   `prompt_text` (str): The input prompt to feed the LLM.\n",
        "        *   `max_new_toks` (int, default=350): The maximum number of new tokens to generate.\n",
        "        *   `temp` (float, default=0.5): The temperature for sampling. Lower values make the output more deterministic; higher values increase randomness.\n",
        "        *   `top_p_val` (float, default=0.9): The nucleus sampling threshold (top-p). Considers only the most likely tokens whose cumulative probability exceeds this value.\n",
        "    *   **Dependencies:** Uses the global `model` and `tokenizer` loaded previously. Includes a check for `LLM_LOAD_SUCCESS`.\n",
        "    *   **Logic:**\n",
        "        1.  **LLM Check:** Returns an error message if the LLM didn't load successfully.\n",
        "        2.  **Chat Templating:** Formats the `prompt_text` into the required structure for the Gemma 2 instruction-tuned model using `tokenizer.apply_chat_template`. This adds necessary role tags (e.g., `<start_of_turn>user\\n...<end_of_turn>\\n<start_of_turn>model\\n`).\n",
        "        3.  **Pad Token Handling:** Ensures the tokenizer has a `pad_token` set (often defaulting to `eos_token` if missing), which is required for batching and consistent generation behavior.\n",
        "        4.  **Input Tensor:** Tokenizes the formatted prompt and converts it to PyTorch tensors (`return_tensors=\"pt\"`), placing them on the appropriate device (`\"cuda\"`).\n",
        "        5.  **Generation Parameters:** Defines a dictionary `generation_params` containing settings like `max_new_tokens`, sampling parameters (`do_sample`, `temperature`, `top_p`), and special token IDs (`eos_token_id`, `pad_token_id`). `use_cache=True` enables the key-value cache for faster generation.\n",
        "        6.  **Inference Mode:** Wraps the generation call in `with torch.inference_mode():` to disable gradient calculations, reducing memory usage and speeding up inference.\n",
        "        7.  **Generate Call:** Calls `model.generate()` with the input tensors and generation parameters.\n",
        "        8.  **Decoding:** Decodes the generated token IDs back into text using `tokenizer.batch_decode`. Crucially, it slices the output tensor (`outputs[:, inputs.shape[1]:]`) to decode *only the newly generated tokens*, excluding the input prompt. `skip_special_tokens=True` removes tokens like `<eos>`.\n",
        "        9.  **Return Value:** Returns the cleaned, generated text string.\n",
        "        10. **Error Handling:** Includes a `try...except` block to catch errors during the generation process itself (e.g., CUDA Out-of-Memory) and returns an error message.\n",
        "\n",
        "## Context & Importance\n",
        "\n",
        "This snippet focuses on the \"Generation\" part of a potential RAG system.\n",
        "*   Loading the LLM via **Unsloth** with **4-bit quantization** is key to making a powerful model like Gemma 2 9B feasible on consumer/modest hardware by significantly reducing VRAM requirements and potentially speeding up inference.\n",
        "*   The prerequisite checks ensure that this resource-intensive step is only attempted if the environment setup and retrieval components are likely functional.\n",
        "*   The `generate_text_unsloth` function provides a clean, reusable interface for interacting with the LLM, incorporating best practices like chat templating, inference mode, and specific decoding of new tokens. This function will likely be called later, feeding it prompts constructed using the context retrieved by `retrieve_and_filter_context` (from Snippet 3).\n",
        "*   The success of this snippet, indicated by `LLM_LOAD_SUCCESS`, is critical for the final text generation capability of the application."
      ],
      "metadata": {
        "id": "1Rf1DghxAlls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Snippet 4 (Revised for Unsloth): Load LLM ===\n",
        "print(\"\\n--- Running Snippet 4 (Revised for Unsloth): Load LLM ---\")\n",
        "if not ('INSTALL_SUCCESS' in globals() and INSTALL_SUCCESS):\n",
        "    print(\"Error: Base libraries did not install successfully. Halting.\")\n",
        "    # exit()\n",
        "if not (EMBEDDING_LOAD_SUCCESS and INDEX_LOAD_SUCCESS):\n",
        "    print(\"Error: Embedding model or index failed to load. Halting LLM load.\")\n",
        "    # exit()\n",
        "\n",
        "print(f\"\\nLoading LLM: {LLM_MODEL_ID} using Unsloth...\")\n",
        "print(\"This may take several minutes and significant VRAM...\")\n",
        "max_seq_length = 4096 # Can adjust based on VRAM\n",
        "dtype = None        # Auto-detect (e.g., Bfloat16 on Ampere+)\n",
        "load_in_4bit = True # Use 4-bit quantization\n",
        "model = None\n",
        "tokenizer = None\n",
        "\n",
        "try:\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=LLM_MODEL_ID,\n",
        "        max_seq_length=max_seq_length,\n",
        "        dtype=dtype,\n",
        "        load_in_4bit=load_in_4bit,\n",
        "        # token = \"hf_...\", # Add huggingface token if needed\n",
        "    )\n",
        "    print(\"Unsloth FastLanguageModel loaded successfully.\")\n",
        "    LLM_LOAD_SUCCESS = True\n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR loading LLM with Unsloth: {e}\")\n",
        "    LLM_LOAD_SUCCESS = False\n",
        "\n",
        "# --- Define a unified generation function ---\n",
        "def generate_text_unsloth(prompt_text, max_new_toks=350, temp=0.5, top_p_val=0.9):\n",
        "    \"\"\"Generates text using the loaded Unsloth model.\"\"\"\n",
        "    global model, tokenizer # Ensure access to loaded model/tokenizer\n",
        "    if not LLM_LOAD_SUCCESS or model is None or tokenizer is None:\n",
        "        print(\"LLM not loaded successfully, cannot generate text.\")\n",
        "        return \"Error: LLM not available.\"\n",
        "\n",
        "    # Using Gemma 2 chat template (applied by tokenizer.apply_chat_template)\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
        "    # Ensure tokenizer has pad_token set if it doesn't naturally\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        # print(\"Warning: pad_token was None, setting to eos_token.\") # Can uncomment for debug\n",
        "\n",
        "    try:\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(\"cuda\") # Ensure input is on CUDA device\n",
        "\n",
        "        generation_params = {\n",
        "            \"max_new_tokens\": max_new_toks,\n",
        "            \"use_cache\": True,\n",
        "            \"do_sample\": True,\n",
        "            \"temperature\": temp,\n",
        "            \"top_p\": top_p_val,\n",
        "            \"eos_token_id\": tokenizer.eos_token_id,\n",
        "            \"pad_token_id\": tokenizer.pad_token_id,\n",
        "        }\n",
        "\n",
        "        # print(f\"\\n--- Generating (max_new_tokens={max_new_toks}, temp={temp}, top_p={top_p_val}) ---\") # Can uncomment for debug\n",
        "        # print(f\"Input prompt (templated): {tokenizer.decode(inputs[0])}\") # Debugging\n",
        "\n",
        "        with torch.inference_mode(): # Ensure inference mode for efficiency\n",
        "            outputs = model.generate(input_ids=inputs, **generation_params)\n",
        "\n",
        "        # Decode only the newly generated part\n",
        "        decoded_output = tokenizer.batch_decode(outputs[:, inputs.shape[1]:], skip_special_tokens=True)[0]\n",
        "        # print(\"--- Generation Complete ---\") # Can uncomment for debug\n",
        "        return decoded_output.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during text generation: {e}\")\n",
        "        # Consider more specific error handling if needed (e.g., CUDA OOM)\n",
        "        return f\"Error during generation: {e}\"\n",
        "\n",
        "print(\"Defined unified generation function 'generate_text_unsloth'.\")\n",
        "print(\"--- Finished Snippet 4 (Revised for Unsloth) ---\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T11:34:18.535159Z",
          "iopub.execute_input": "2025-04-23T11:34:18.535734Z",
          "iopub.status.idle": "2025-04-23T11:34:46.239694Z",
          "shell.execute_reply.started": "2025-04-23T11:34:18.535715Z",
          "shell.execute_reply": "2025-04-23T11:34:46.239036Z"
        },
        "colab": {
          "referenced_widgets": [
            "ab3302a29af24ad9b0a8419c31257ff2",
            "b515104f050b4a3a9b40daaa4f16ec49",
            "abd86e202d584ba0a9e91533217599f1",
            "0ac5a181acf84c469286060275910f23",
            "2bae6769827e4cd69f02dc732a952ea8",
            "4efb9105e7b440cfb211b4aa217bb1c8"
          ]
        },
        "id": "_2Ba9gHvAllt",
        "outputId": "2fde9241-c997-4bad-f60b-897e6371e1da"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n--- Running Snippet 4 (Revised for Unsloth): Load LLM ---\n\nLoading LLM: unsloth/gemma-2-9b-it-bnb-4bit using Unsloth...\nThis may take several minutes and significant VRAM...\n==((====))==  Unsloth 2025.3.19: Fast Gemma2 patching. Transformers: 4.51.1.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/6.13G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab3302a29af24ad9b0a8419c31257ff2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b515104f050b4a3a9b40daaa4f16ec49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abd86e202d584ba0a9e91533217599f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ac5a181acf84c469286060275910f23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bae6769827e4cd69f02dc732a952ea8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4efb9105e7b440cfb211b4aa217bb1c8"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Unsloth FastLanguageModel loaded successfully.\nDefined unified generation function 'generate_text_unsloth'.\n--- Finished Snippet 4 (Revised for Unsloth) ---\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Snippet 5 (Revised): Main Logic Function ---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This snippet defines the core orchestration function, `rewrite_and_verify`. Its primary goal is to take an input text (`original_text`) and iteratively rewrite it until it is deemed safe (spoiler-free) for a user who has only watched \"The Wire\" up to a specific point (`user_progress_str`). It employs a multi-stage approach involving Retrieval-Augmented Generation (RAG), LLM-based verification, and LLM-based rewriting, incorporating anti-hallucination checks and progressively stricter criteria.\n",
        "\n",
        "## Key Actions\n",
        "\n",
        "1.  **Prerequisite & Input Validation:**\n",
        "    *   Checks if the necessary components (LLM, Embedding Model, Index) from previous snippets loaded successfully (`LLM_LOAD_SUCCESS`, etc.). Returns an error immediately if not.\n",
        "    *   Parses the `user_progress_str` using the `parse_progress` helper function. Returns an error if the format is invalid.\n",
        "    *   Logs the start of the process and the user's progress point.\n",
        "\n",
        "2.  **Inner Helper Function Definition (`find_potential_spoiler_spans`):**\n",
        "    *   Defines a local helper function to locate potential spoiler sentences within a given text based on phrases identified by RAG.\n",
        "    *   **Input:** `text`, `spoiler_phrases` (list of strings, potentially prefixed with \"(S E)\").\n",
        "    *   **Logic:**\n",
        "        *   Attempts to extract the core text from `spoiler_phrases` (removing `(SxE)`).\n",
        "        *   Performs case-insensitive search (`find`) for each phrase within the `text`.\n",
        "        *   For each match, determines the start and end indices of the surrounding sentence using punctuation (`.!? `) as delimiters. Handles edge cases (start/end of text).\n",
        "        *   Includes logic to avoid adding duplicate or highly overlapping spans if multiple spoiler phrases match the same sentence segment.\n",
        "    *   **Output:** A list of tuples `(start_index, end_index, triggering_phrase)` representing identified potential spoiler sentence locations.\n",
        "\n",
        "3.  **Initial Context Gathering (RAG):**\n",
        "    *   Calls `retrieve_and_filter_context` with a *permissive* distance threshold (`0.65`) and high result count (`n_results=8`) to gather `safe_context`. This context represents events confirmed to be *at or before* the user's progress and is used later to help the LLM avoid misidentifying past events as spoilers.\n",
        "    *   Calls `retrieve_and_filter_context` again with a slightly stricter threshold (`0.58`, `n_results=6`) on the `original_text` to get an initial list of `spoilers_to_remove` (potential spoilers present in the input).\n",
        "\n",
        "4.  **Initialization for Iterative Process:**\n",
        "    *   Sets up history lists (`candidates_history`, `feedback_history`) and a list to track spoiler locations (`spoiler_locations`).\n",
        "    *   Initializes `current_candidate_text` with the `original_text`.\n",
        "    *   Defines a list of `thresholds` (`[0.58, 0.54, 0.50, 0.46]`) representing progressively stricter RAG distance thresholds to be used in verification across attempts.\n",
        "\n",
        "5.  **Main Refinement Loop (`for attempt in range(1, max_attempts + 1):`)**:\n",
        "    *   Iterates up to `max_attempts`.\n",
        "    *   **Verification Phase:**\n",
        "        *   Selects the `current_threshold` based on the attempt number.\n",
        "        *   **RAG for Verification Hints:** Runs `retrieve_and_filter_context` on the `current_candidate_text` using `current_threshold` to identify potential spoilers (`spoilers_in_candidate`) *in the current version* of the text.\n",
        "        *   **Locate Spoilers:** Uses `find_potential_spoiler_spans` to pinpoint where these hints might appear in the `current_candidate_text`. Stores these locations with the attempt number.\n",
        "        *   **Build Verifier Prompt:** Constructs a detailed prompt for the LLM (`generate_text_unsloth`):\n",
        "            *   Sets the persona (spoiler detector).\n",
        "            *   Specifies user progress (`S{user_season}E{user_episode}`).\n",
        "            *   Includes attempt-specific focus instructions (`verification_focus`) that get stricter over attempts.\n",
        "            *   Provides the `current_candidate_text`.\n",
        "            *   Includes **CRITICAL ANTI-HALLUCINATION INSTRUCTIONS** (e.g., never flag past events, ground in text, uncertainty=pass).\n",
        "            *   Emphasizes **Timeline Awareness** (what is safe vs. spoiler).\n",
        "            *   Includes `safe_context_info` (previously retrieved safe events).\n",
        "            *   Includes `spoilers_in_candidate` (RAG hints).\n",
        "            *   Specifies the required output format (\"PASS\" or \"FAIL: [reason]\").\n",
        "        *   **Call Verifier LLM:** Executes `generate_text_unsloth` with a **low temperature** (`0.08`) for deterministic verification.\n",
        "        *   **Parse Primary Result:** Checks if the result indicates a \"FAIL\".\n",
        "        *   **Secondary Verification (Anti-Hallucination Check):**\n",
        "            *   If the primary verification indicated \"FAIL\".\n",
        "            *   Extracts the claimed spoiler reason/text.\n",
        "            *   Builds a **Secondary Verification Prompt** asking a timeline expert LLM to *specifically* check if the claimed spoiler is *actually* from after the user's progress, providing stringent anti-hallucination rules.\n",
        "            *   Calls `generate_text_unsloth` with a **very low temperature** (`0.05`).\n",
        "            *   If the secondary check returns \"FALSE ALARM\", it overrides the primary verification result to \"PASS\".\n",
        "    *   **Process Verification Result (Post-Secondary Check):**\n",
        "        *   **If Passed:** If the current `attempt` is the `max_attempts`, return the text and `True`. Otherwise, log passage and continue to the next, stricter attempt.\n",
        "        *   **If Failed (Confirmed):**\n",
        "            *   Extract feedback (the reason for failure) from the verifier's response.\n",
        "            *   Append feedback to `feedback_history`.\n",
        "            *   **Rewriting Phase:**\n",
        "                *   Build **Rewriter Prompt** for the LLM:\n",
        "                    *   Sets persona (expert editor).\n",
        "                    *   Specifies user progress.\n",
        "                    *   Includes attempt-specific `rewrite_strategy` (e.g., rewrite major spoilers vaguely, preserve names/locations).\n",
        "                    *   Provides the `formatted_feedback` (including recent problematic spans found in this attempt).\n",
        "                    *   Reiterates timeline rules (preserve past, modify future).\n",
        "                    *   Includes `safe_context_info`.\n",
        "                    *   Provides detailed instructions (preserve names, rewrite vague, maintain length, focus on feedback).\n",
        "                    *   Includes the `current_candidate_text` to be rewritten.\n",
        "                *   Estimate `max_new_toks` for the rewriter based on input length.\n",
        "                *   Call Rewriter LLM (`generate_text_unsloth`) with a slightly higher temperature (`0.2` to `0.28` depending on attempt) to allow for creative rewriting.\n",
        "                *   Handle potential LLM errors during rewrite gracefully (return the previous candidate text and `False`).\n",
        "                *   Update `current_candidate_text` with the rewritten version.\n",
        "                *   Add the *previous* candidate text to `candidates_history`.\n",
        "                *   Continue to the next loop iteration with the rewritten text.\n",
        "\n",
        "6.  **Post-Loop Final Verification:**\n",
        "    *   This section executes only if the loop completes all `max_attempts` *without* passing the verification on the final attempt.\n",
        "    *   Performs one last verification using a strict RAG threshold (`0.46`).\n",
        "    *   Builds a **Final Verifier Prompt** similar to the loop's verifier but emphasizing it's the final check.\n",
        "    *   Calls Verifier LLM with a low temperature (`0.05`).\n",
        "    *   Includes a **Final Secondary Verification** step (identical logic to the loop's secondary check, using temp `0.03`) if the final primary verification fails, to catch potential last-minute hallucinations.\n",
        "    *   Based on the potentially corrected final verification result, returns the final `current_candidate_text` along with `True` if it passed, or `False` if spoilers are still detected.\n",
        "\n",
        "## Context & Importance\n",
        "\n",
        "This function is the heart of the spoiler-handling application. It implements a sophisticated, multi-step process designed to be robust:\n",
        "*   **Iterative Refinement:** Instead of a single rewrite attempt, it progressively tries to fix the text, using increasingly strict criteria.\n",
        "*   **RAG Integration:** Uses vector search (RAG) to find potentially relevant passages (both safe context and potential spoilers) to guide the LLMs.\n",
        "*   **LLM Verification:** Leverages an LLM specifically prompted for verification, focusing on timeline accuracy.\n",
        "*   **LLM Rewriting:** Uses another LLM call, guided by the verifier's feedback, to perform targeted rewrites.\n",
        "*   **Anti-Hallucination:** Incorporates multiple layers of checks (strict prompting, RAG for grounding, secondary verification LLM calls) specifically designed to prevent the LLMs from incorrectly flagging non-spoilers or inventing timeline issues.\n",
        "*   **State Management:** Keeps track of text versions and feedback history.\n",
        "\n"
      ],
      "metadata": {
        "id": "hnLV97oQAllt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Snippet 5 (Revised): Main Logic Function ===\n",
        "print(\"\\n--- Running Snippet 5: Defining Main Rewrite & Verify Logic ---\")\n",
        "\n",
        "def rewrite_and_verify(original_text, user_progress_str, max_attempts=4):\n",
        "    \"\"\"\n",
        "    Progressive refinement approach with enhanced past-episode recognition and\n",
        "    anti-hallucination safeguards for spoiler detection.\n",
        "    \"\"\"\n",
        "    # Make sure models/index are loaded before proceeding\n",
        "    if not (LLM_LOAD_SUCCESS and EMBEDDING_LOAD_SUCCESS and INDEX_LOAD_SUCCESS):\n",
        "        print(\"Error: Required models or index not loaded.\")\n",
        "        return \"Error: Required models or index not loaded.\", False\n",
        "\n",
        "    user_season, user_episode = parse_progress(user_progress_str)\n",
        "    if user_season is None or user_episode is None:\n",
        "        print(f\"Error: Invalid user progress format '{user_progress_str}'.\")\n",
        "        return f\"Error: Invalid user progress format '{user_progress_str}'.\", False\n",
        "\n",
        "    print(f\"\\n=== Starting Spoiler Rewriting for User at S{user_season}E{user_episode} ===\")\n",
        "    print(f\"Original Text (first 150 chars): {original_text[:150]}...\") # Shortened print\n",
        "\n",
        "    # --- Helper function defined inside for locality ---\n",
        "    def find_potential_spoiler_spans(text, spoiler_phrases):\n",
        "        spans = []\n",
        "        text_lower = text.lower() # Lowercase text once\n",
        "        for phrase in spoiler_phrases:\n",
        "            # Extract key part if separator exists, handle potential errors\n",
        "            try:\n",
        "                key_phrase = phrase.split(') ', 1)[1] if ') ' in phrase else phrase # Try to get text after (SxE)\n",
        "                key_phrase = key_phrase.split(' - ')[0].strip() # Get part before optional ' - '\n",
        "            except IndexError:\n",
        "                 key_phrase = phrase.strip() # Fallback to using the whole phrase if split fails\n",
        "\n",
        "            if not key_phrase: continue # Skip empty phrases\n",
        "\n",
        "            phrase_lower = key_phrase.lower()\n",
        "\n",
        "            # Find all occurrences, not just the first\n",
        "            start_idx = 0\n",
        "            while start_idx < len(text_lower):\n",
        "                found_idx = text_lower.find(phrase_lower, start_idx)\n",
        "                if found_idx == -1:\n",
        "                    break # Not found in the rest of the text\n",
        "\n",
        "                # Get surrounding context (sentence) - slightly improved logic\n",
        "                # Find start of sentence (previous period/!/? + space or start of text)\n",
        "                sent_start = max(0, text_lower.rfind('. ', 0, found_idx) + 2)\n",
        "                sent_start = max(sent_start, text_lower.rfind('! ', 0, found_idx) + 2)\n",
        "                sent_start = max(sent_start, text_lower.rfind('? ', 0, found_idx) + 2)\n",
        "                # Handle case where sentence starts the text\n",
        "                if text_lower.rfind('. ', 0, found_idx) == -1 and \\\n",
        "                   text_lower.rfind('! ', 0, found_idx) == -1 and \\\n",
        "                   text_lower.rfind('? ', 0, found_idx) == -1 and found_idx > 0:\n",
        "                    sent_start = 0 # Likely start of the text\n",
        "\n",
        "                # Find end of sentence (next period/!/? or end of text)\n",
        "                sent_end = text_lower.find('. ', found_idx)\n",
        "                sent_end_q = text_lower.find('? ', found_idx)\n",
        "                sent_end_e = text_lower.find('! ', found_idx)\n",
        "\n",
        "                # Find the earliest sentence end marker\n",
        "                possible_ends = [e for e in [sent_end, sent_end_q, sent_end_e] if e != -1]\n",
        "                if not possible_ends:\n",
        "                    sent_end = len(text) # End of text if no marker found\n",
        "                else:\n",
        "                    sent_end = min(possible_ends) + 1 # Include the punctuation\n",
        "\n",
        "                # Avoid adding duplicate spans if multiple search phrases match same text part\n",
        "                is_duplicate = False\n",
        "                for existing_start, existing_end, _ in spans:\n",
        "                    # Check for significant overlap\n",
        "                    if max(sent_start, existing_start) < min(sent_end, existing_end):\n",
        "                         is_duplicate = True\n",
        "                         break\n",
        "                if not is_duplicate:\n",
        "                    spans.append((sent_start, sent_end, key_phrase)) # Store key_phrase found\n",
        "\n",
        "                start_idx = found_idx + 1 # Continue search after this find\n",
        "\n",
        "        return spans\n",
        "    # --- End of inner helper function ---\n",
        "\n",
        "\n",
        "    # Get episode-specific safe context with more results to better understand past episodes\n",
        "    safe_threshold = 0.65 # Very permissive for identifying safe content\n",
        "    # We only need the text part for the prompt, discard potential future spoilers from this call\n",
        "    safe_context, _ = retrieve_and_filter_context(\n",
        "        original_text, user_season, user_episode, n_results=8, distance_threshold=safe_threshold\n",
        "    )\n",
        "    print(f\"Safe Context: Retrieved {len(safe_context)} items about past episodes (up to S{user_season}E{user_episode})\")\n",
        "\n",
        "    # Initial RAG with permissive threshold for spoiler detection\n",
        "    initial_threshold = 0.58 # Permissive starting point\n",
        "    # Here we *do* care about potential spoilers found in the original text\n",
        "    _, spoilers_to_remove = retrieve_and_filter_context(\n",
        "        original_text, user_season, user_episode, n_results=6, distance_threshold=initial_threshold\n",
        "    )\n",
        "    print(f\"Initial RAG (threshold {initial_threshold:.2f}): Found {len(spoilers_to_remove)} potential spoilers.\")\n",
        "\n",
        "    # History tracking\n",
        "    candidates_history = []\n",
        "    feedback_history = []\n",
        "    spoiler_locations = [] # Track where spoilers have been found\n",
        "\n",
        "    # Start with original as first candidate\n",
        "    current_candidate_text = original_text\n",
        "    candidates_history.append(current_candidate_text)\n",
        "\n",
        "    # Progressive threshold tightening - reasonable pace\n",
        "    thresholds = [0.58, 0.54, 0.50, 0.46] # Gradually more strict\n",
        "\n",
        "    # --- Main Loop ---\n",
        "    for attempt in range(1, max_attempts + 1):\n",
        "        print(f\"\\n--- Progressive Refinement: Attempt {attempt}/{max_attempts} ---\")\n",
        "\n",
        "        # Get current threshold\n",
        "        current_threshold = thresholds[min(attempt-1, len(thresholds)-1)]\n",
        "        print(f\"Using verification threshold: {current_threshold:.2f}\")\n",
        "\n",
        "        # --- Verify current candidate ---\n",
        "        print(\">> Calling Verifier LLM for current candidate...\")\n",
        "\n",
        "        # RAG with current threshold - get hints for the verifier\n",
        "        _, spoilers_in_candidate = retrieve_and_filter_context(\n",
        "            current_candidate_text, user_season, user_episode, n_results=5, distance_threshold=current_threshold\n",
        "        )\n",
        "\n",
        "        # Find potential spoiler locations in the *current* text\n",
        "        potential_spans = find_potential_spoiler_spans(current_candidate_text, spoilers_in_candidate)\n",
        "        # Note: spoiler_locations list grows over attempts, could prune old ones if needed\n",
        "        for span in potential_spans:\n",
        "            # Avoid adding duplicates based on text content and attempt\n",
        "            is_new_location = True\n",
        "            for loc in spoiler_locations:\n",
        "                if loc['text'] == current_candidate_text[span[0]:span[1]] and loc['attempt'] == attempt:\n",
        "                    is_new_location = False\n",
        "                    break\n",
        "            if is_new_location:\n",
        "                 spoiler_locations.append({\n",
        "                    'start': span[0],\n",
        "                    'end': span[1],\n",
        "                    'text': current_candidate_text[span[0]:span[1]],\n",
        "                    'phrase': span[2], # Store the RAG phrase that triggered this\n",
        "                    'attempt': attempt\n",
        "                 })\n",
        "\n",
        "\n",
        "        # Format safe context information for the verifier prompt\n",
        "        safe_context_info = \"\"\n",
        "        if safe_context:\n",
        "            safe_context_info = f\"CONFIRMED SAFE EVENTS (up to S{user_season}E{user_episode}):\\n\"\n",
        "            # Limit the number shown to avoid excessive prompt length\n",
        "            safe_context_info += \"\\n\".join([f\"- {item}\" for item in safe_context[:5]])\n",
        "\n",
        "        # Create primary verification prompt\n",
        "        verification_focus = \"\"\n",
        "        if attempt == 1:\n",
        "            verification_focus = f\"Focus ONLY on obvious MAJOR plot spoilers beyond S{user_season}E{user_episode}. DO NOT flag events from S{user_season}E{user_episode} or earlier - these are NOT spoilers.\"\n",
        "        elif attempt == 2:\n",
        "            verification_focus = f\"Focus on clear spoilers about character developments beyond S{user_season}E{user_episode}. CAREFULLY CHECK if events mentioned are from before or after S{user_season}E{user_episode}. DO NOT mark past episodes as spoilers.\"\n",
        "        elif attempt >= 3:\n",
        "            verification_focus = f\"Be thorough but grounded. Clearly distinguish between events before vs. after S{user_season}E{user_episode}. Events up to S{user_season}E{user_episode} are NEVER spoilers. Avoid flagging content unless you're confident it reveals FUTURE events.\"\n",
        "\n",
        "        verifier_prompt = f\"\"\"You are a specialized spoiler detection system for 'The Wire' with timeline awareness.\n",
        "A user has watched ONLY up to Season {user_season}, Episode {user_episode}.\n",
        "Your task is to verify that the text contains NO spoilers from FUTURE episodes (after S{user_season}E{user_episode}).\n",
        "\n",
        "**Current Analysis Phase: {attempt} of {max_attempts}**\n",
        "{verification_focus}\n",
        "\n",
        "**Candidate Text:**\n",
        "{current_candidate_text}\n",
        "\n",
        "**CRITICAL ANTI-HALLUCINATION INSTRUCTIONS:**\n",
        "1. NEVER flag content from S{user_season}E{user_episode} or earlier as spoilers.\n",
        "2. Stay grounded in what the text ACTUALLY says - don't invent connections.\n",
        "3. A name, location, or item alone is NOT a spoiler unless it explicitly reveals future events.\n",
        "4. Content is only a spoiler if it DEFINITELY reveals events beyond S{user_season}E{user_episode}.\n",
        "5. If unsure when an event occurs, DO NOT flag it as a spoiler.\n",
        "6. UNCERTAINTY = PASS (default to passing unless certain it's a future spoiler).\n",
        "\n",
        "**Timeline Awareness:**\n",
        "- Everything up to and including S{user_season}E{user_episode} is SAFE content.\n",
        "- Only content about episodes after S{user_season}E{user_episode} counts as spoilers.\n",
        "- ASK YOURSELF: \"Does this text explicitly reveal events that happen after S{user_season}E{user_episode}?\"\n",
        "\n",
        "{safe_context_info}\n",
        "\n",
        "**System-Flagged Potential Spoilers (Hints for Analysis):**\n",
        "{chr(10).join(f' - {s}' for s in spoilers_in_candidate) if spoilers_in_candidate else ' - None automatically flagged (still review manually)'}\n",
        "\n",
        "**Output Format:**\n",
        "- If no FUTURE spoilers: Respond with \"PASS\"\n",
        "- If FUTURE spoilers found: Respond with \"FAIL: [Quote the exact spoiler text and specify which future episode it spoils, e.g., S1E10]\"\n",
        "\n",
        "Provide your analysis.\n",
        "\"\"\"\n",
        "        # Use lower temp for more deterministic verification\n",
        "        verification_result = generate_text_unsloth(verifier_prompt, temp=0.08, max_new_toks=200)\n",
        "\n",
        "        print(f\"Verifier Result: {verification_result[:150]}...\" if len(verification_result) > 150 else verification_result) # Shortened print\n",
        "\n",
        "        verification_result_clean = verification_result.strip().upper()\n",
        "        is_primary_fail = not verification_result_clean.startswith(\"PASS\") and \"FAIL\" in verification_result_clean\n",
        "\n",
        "        # --- Secondary Verification (Anti-Hallucination) if primary failed ---\n",
        "        if is_primary_fail:\n",
        "            print(\">> Running secondary verification to prevent hallucination...\")\n",
        "\n",
        "            # Extract the claimed spoiler for double-checking\n",
        "            claimed_spoiler_text = \"Unknown\"\n",
        "            if \":\" in verification_result:\n",
        "                # Take everything after the first colon as the claimed issue\n",
        "                claimed_spoiler_text = verification_result.split(\":\", 1)[1].strip()\n",
        "            else:\n",
        "                 # If no colon, use the whole result as context (less ideal)\n",
        "                claimed_spoiler_text = verification_result.strip()\n",
        "\n",
        "            # Secondary verification prompt\n",
        "            secondary_prompt = f\"\"\"You are a specialized 'The Wire' timeline expert.\n",
        "CRITICAL: Double-check if this alleged spoiler actually reveals events after Season {user_season}, Episode {user_episode}.\n",
        "\n",
        "**Text being reviewed:**\n",
        "{current_candidate_text}\n",
        "\n",
        "**Claimed spoiler / Context:**\n",
        "{claimed_spoiler_text}\n",
        "\n",
        "**TIMELINE VERIFICATION INSTRUCTIONS:**\n",
        "1. Very carefully determine if this content TRULY reveals events after S{user_season}E{user_episode}.\n",
        "2. Content about events in or before S{user_season}E{user_episode} is NEVER a spoiler.\n",
        "3. A name, location, or item alone is NOT a spoiler unless it explicitly reveals future events.\n",
        "4. General themes or vague statements are NOT spoilers without specific plot revelations.\n",
        "5. Be extremely careful not to hallucinate timeline information.\n",
        "\n",
        "{safe_context_info}\n",
        "\n",
        "Output ONLY:\n",
        "\"CONFIRMED SPOILER\" if you are certain this reveals events after S{user_season}E{user_episode}.\n",
        "\"FALSE ALARM\" if this content is about events before or during S{user_season}E{user_episode}, or if it's too vague to be a spoiler.\n",
        "\"\"\"\n",
        "            # Use very low temp for deterministic secondary check\n",
        "            secondary_verification = generate_text_unsloth(secondary_prompt, temp=0.05, max_new_toks=50)\n",
        "            print(f\"Secondary verification result: {secondary_verification.strip()}\")\n",
        "\n",
        "            # Override the verification result if it was a false alarm\n",
        "            if \"FALSE ALARM\" in secondary_verification.upper():\n",
        "                verification_result = \"PASS (corrected after secondary verification)\"\n",
        "                verification_result_clean = \"PASS\" # Update clean result too\n",
        "                is_primary_fail = False # Mark as no longer failed\n",
        "                print(\">> Verification corrected: False alarm detected\")\n",
        "            # else: Confirmed spoiler, proceed with feedback extraction\n",
        "\n",
        "        # --- Process Verification Result ---\n",
        "        if verification_result_clean.startswith(\"PASS\"):\n",
        "             # If it passed AND it's the last attempt, we're done\n",
        "             if attempt == max_attempts:\n",
        "                 print(f\"\\n=== Verification PASSED on final attempt {attempt} ===\")\n",
        "                 # No need for final check if it passed the last attempt's strictness\n",
        "                 return current_candidate_text, True\n",
        "             else:\n",
        "                 # Passed this phase, continue to next attempt for stricter check\n",
        "                 print(f\"Phase {attempt} verification passed, proceeding to stricter phase {attempt+1}\")\n",
        "                 # Loop continues naturally\n",
        "        else: # This means primary verification failed AND secondary check confirmed it (or secondary check wasn't needed)\n",
        "            # Extract feedback\n",
        "            if verification_result.strip().upper().startswith(\"FAIL:\"):\n",
        "                feedback_content = verification_result.strip()[len(\"FAIL:\"):].strip()\n",
        "            else:\n",
        "                # Use the full result if format is unexpected, better than no feedback\n",
        "                feedback_content = verification_result.strip()\n",
        "\n",
        "            feedback_history.append(feedback_content)\n",
        "            print(f\"Verification identified issues. Feedback: {feedback_content[:100]}...\") # Shortened print\n",
        "\n",
        "            # --- LLM Rewriter for targeted rewriting ---\n",
        "            print(\">> Calling Rewriter LLM for targeted rewriting...\")\n",
        "\n",
        "            # Define rewriting strategy based on attempt\n",
        "            rewrite_strategy = \"\"\n",
        "            rewrite_temp = 0.2 # Default temp\n",
        "            if attempt == 1:\n",
        "                rewrite_temp = 0.2\n",
        "                rewrite_strategy = f\"REWRITE major spoilers to make future events vaguer while PRESERVING names, locations, and items. DO NOT modify content about events from S{user_season}E{user_episode} or earlier.\"\n",
        "            elif attempt == 2:\n",
        "                rewrite_temp = 0.25\n",
        "                rewrite_strategy = f\"REWRITE character development spoilers by making outcomes and future events vaguer. RETAIN all names, places, and objects but make their future actions less specific. DO NOT change past episode content.\"\n",
        "            elif attempt >= 3:\n",
        "                rewrite_temp = 0.28\n",
        "                rewrite_strategy = f\"THOROUGHLY REWRITE any future plot hints while PRESERVING all names, places, and references. Replace specific outcomes with more general descriptions. ABSOLUTELY PRESERVE all content about S{user_season}E{user_episode} or earlier.\"\n",
        "\n",
        "            # Format feedback, potentially including recent locations\n",
        "            formatted_feedback = feedback_content\n",
        "            # Show only locations found in *this* attempt for clarity\n",
        "            current_attempt_locations = [loc for loc in spoiler_locations if loc['attempt'] == attempt]\n",
        "            if current_attempt_locations:\n",
        "                formatted_feedback += \"\\n\\nProblematic sections identified in this pass may include:\"\n",
        "                # Show max 3 most recent locations from *this* attempt\n",
        "                for loc in current_attempt_locations[-3:]:\n",
        "                     formatted_feedback += f\"\\n- \\\"{loc['text']}\\\" (related to '{loc['phrase']}')\"\n",
        "\n",
        "            # Build rewriter prompt\n",
        "            rewriter_prompt = f\"\"\"You are an expert TV show content editor specializing in spoiler management for 'The Wire'.\n",
        "Your task is to rewrite content to be safe for a viewer who has only watched up to Season {user_season}, Episode {user_episode}.\n",
        "\n",
        "**CURRENT PHASE: {attempt} of {max_attempts} - PRECISE REWRITING**\n",
        "{rewrite_strategy}\n",
        "\n",
        "**FEEDBACK FROM VERIFICATION (Needs Addressing):**\n",
        "{formatted_feedback}\n",
        "\n",
        "**TIMELINE PRESERVATION RULES:**\n",
        "1. Content about S{user_season}E{user_episode} and earlier episodes MUST be preserved exactly.\n",
        "2. Only modify content that reveals events AFTER S{user_season}E{user_episode}.\n",
        "3. When rewriting future events, preserve names but make outcomes vaguer.\n",
        "\n",
        "**SAFE CONTEXT (Confirmed to be at/before S{user_season}E{user_episode}):**\n",
        "{safe_context_info if safe_context_info else 'None provided'}\n",
        "\n",
        "**REWRITING INSTRUCTIONS:**\n",
        "1. PRESERVE NAMES, LOCATIONS, AND ITEMS - these are NOT spoilers by themselves.\n",
        "2. DO NOT REMOVE information - instead REWRITE it to be vaguer about future outcomes.\n",
        "3. Try to maintain text length and information density where possible without spoiling.\n",
        "4. Focus on precision - modify only what needs changing based on feedback.\n",
        "5. Apply the phase-specific strategy: {rewrite_strategy}.\n",
        "6. CRITICAL: If a character faces major changes beyond S{user_season}E{user_episode}, don't remove their name but rewrite to hide their specific fate.\n",
        "\n",
        "**Current Text (Needs Rewriting):**\n",
        "{current_candidate_text}\n",
        "\n",
        "Rewrite the text addressing the feedback. Output ONLY the rewritten text with no explanations.\n",
        "\"\"\"\n",
        "            # Estimate max tokens based on current text length + some buffer\n",
        "            estimated_max_tokens = max(200, int(len(current_candidate_text.split()) * 1.5))\n",
        "\n",
        "            generated_text = generate_text_unsloth(\n",
        "                rewriter_prompt,\n",
        "                temp=rewrite_temp,\n",
        "                max_new_toks=estimated_max_tokens\n",
        "            )\n",
        "\n",
        "            # Handle LLM errors during rewrite\n",
        "            if generated_text.startswith(\"Error:\"):\n",
        "                print(f\"Rewriter LLM failed: {generated_text}\")\n",
        "                # Fall back to the text *before* this failed rewrite attempt\n",
        "                if len(candidates_history) > 1: # Ensure there's a previous state\n",
        "                    return candidates_history[-1], False # Return the last known good state\n",
        "                else:\n",
        "                    return original_text, False # Or just return original if first attempt failed badly\n",
        "                # return \"Error during rewriting process.\", False # Old return\n",
        "\n",
        "            # Store history *before* updating current text\n",
        "            candidates_history.append(current_candidate_text) # Save the text *before* rewrite\n",
        "            current_candidate_text = generated_text # Update to the newly rewritten text\n",
        "            print(f\"Rewriter Output (first 150 chars): {current_candidate_text[:150]}...\") # Shortened print\n",
        "            # Loop continues to next attempt\n",
        "\n",
        "    # --- Post-Loop Final Verification (Only if loop finished without passing on last attempt) ---\n",
        "    # Note: If loop finished because it passed on attempt == max_attempts, it already returned True above.\n",
        "    # This section runs if the loop completed all attempts and the last one *failed* primary verification.\n",
        "    print(\"\\n>> Final Verification Check (After Max Attempts)...\")\n",
        "    final_threshold = 0.46 # Strict but reasonable final RAG threshold\n",
        "    _, final_spoilers = retrieve_and_filter_context(\n",
        "        current_candidate_text, user_season, user_episode, n_results=5, distance_threshold=final_threshold\n",
        "    )\n",
        "\n",
        "    # Re-use safe context info from earlier\n",
        "    final_verifier_prompt = f\"\"\"You are a specialized spoiler detection system for 'The Wire' with timeline awareness.\n",
        "A user has watched ONLY up to Season {user_season}, Episode {user_episode}.\n",
        "This is the FINAL verification check for the candidate text after multiple rewrite attempts.\n",
        "\n",
        "**Candidate Text:**\n",
        "{current_candidate_text}\n",
        "\n",
        "**CRITICAL ANTI-HALLUCINATION INSTRUCTIONS:**\n",
        "1. NEVER flag content from S{user_season}E{user_episode} or earlier as spoilers.\n",
        "2. Stay grounded in what the text ACTUALLY says - don't invent connections.\n",
        "3. A name, location, or item alone is NOT a spoiler unless it explicitly reveals future events.\n",
        "4. Content is only a spoiler if it DEFINITELY reveals events beyond S{user_season}E{user_episode}.\n",
        "5. If unsure when an event occurs, DO NOT flag it as a spoiler.\n",
        "6. UNCERTAINTY = PASS (default to passing unless certain it's a future spoiler).\n",
        "\n",
        "**Timeline Awareness:**\n",
        "- Everything up to and including S{user_season}E{user_episode} is SAFE content.\n",
        "- Only content about episodes after S{user_season}E{user_episode} counts as spoilers.\n",
        "\n",
        "{safe_context_info}\n",
        "\n",
        "**System-Flagged Potential Final Spoilers (Hints for Analysis):**\n",
        "{chr(10).join(f'- {s}' for s in final_spoilers) if final_spoilers else ' - None automatically flagged (still review manually)'}\n",
        "\n",
        "**Final Verification Instructions:**\n",
        "- Be thorough but fair in your analysis.\n",
        "- Flag ONLY content that explicitly reveals events AFTER S{user_season}E{user_episode}.\n",
        "- NAMES, LOCATIONS, and ITEMS alone are NOT spoilers unless they directly reveal future plot points.\n",
        "\n",
        "Output ONLY \"PASS\" if safe for S{user_season}E{user_episode} viewer, or \"FAIL: [reason]\" if spoilers remain.\n",
        "\"\"\"\n",
        "    # Use low temp for final check\n",
        "    final_verification = generate_text_unsloth(final_verifier_prompt, temp=0.05, max_new_toks=150)\n",
        "    final_verification_clean = final_verification.strip().upper()\n",
        "\n",
        "    is_final_fail = not final_verification_clean.startswith(\"PASS\") and \"FAIL\" in final_verification_clean\n",
        "\n",
        "    # --- Final Secondary Verification if final primary failed ---\n",
        "    if is_final_fail:\n",
        "        print(\">> Running final secondary verification to prevent hallucination...\")\n",
        "        # Extract claimed spoiler\n",
        "        claimed_spoiler = final_verification.split(\":\", 1)[1].strip() if \":\" in final_verification else final_verification\n",
        "\n",
        "        secondary_final_prompt = f\"\"\"You are a specialized 'The Wire' timeline expert with anti-hallucination safeguards.\n",
        "CRITICAL: Double-check if this alleged spoiler actually reveals events after Season {user_season}, Episode {user_episode}.\n",
        "\n",
        "**Text being reviewed:**\n",
        "{current_candidate_text}\n",
        "\n",
        "**Claimed spoiler / Context:**\n",
        "{claimed_spoiler}\n",
        "\n",
        "**ANTI-HALLUCINATION CHECKLIST:**\n",
        "1. Does the text EXPLICITLY mention events that happen AFTER S{user_season}E{user_episode}?\n",
        "2. Are you CERTAIN this isn't about events in or before S{user_season}E{user_episode}?\n",
        "3. Does it reveal SPECIFIC future outcomes rather than general themes?\n",
        "4. Are you avoiding reading between the lines or making unwarranted inferences?\n",
        "5. Are you certain you're not confusing character mentions with spoilers?\n",
        "\n",
        "{safe_context_info}\n",
        "\n",
        "Output ONLY:\n",
        "\"CONFIRMED SPOILER\" if you are certain this reveals specific events after S{user_season}E{user_episode}.\n",
        "\"FALSE ALARM\" if this content is about events in or before S{user_season}E{user_episode}, or if it's too vague to be a spoiler.\n",
        "\"\"\"\n",
        "        # Use very low temp\n",
        "        secondary_final = generate_text_unsloth(secondary_final_prompt, temp=0.03, max_new_toks=50)\n",
        "        print(f\"Final secondary verification result: {secondary_final.strip()}\")\n",
        "\n",
        "        # Override final result if false alarm detected\n",
        "        if \"FALSE ALARM\" in secondary_final.upper():\n",
        "            final_verification = \"PASS (corrected after final verification)\"\n",
        "            final_verification_clean = \"PASS\"\n",
        "            is_final_fail = False # Mark as no longer failed\n",
        "            print(\">> Final verification corrected: False alarm detected\")\n",
        "        # else: Final fail is confirmed\n",
        "\n",
        "    # Return final result based on potentially corrected final verification\n",
        "    if final_verification_clean.startswith(\"PASS\"):\n",
        "        print(\"\\n=== Final Verification PASSED (After Loop Completion) ===\")\n",
        "        return current_candidate_text, True\n",
        "    else:\n",
        "        print(f\"\\n=== Final Verification FAILED (After Loop Completion): {final_verification} ===\")\n",
        "        return current_candidate_text, False\n",
        "\n",
        "# --- End of rewrite_and_verify function definition ---\n",
        "\n",
        "print(\"\\nDefined rewrite_and_verify function.\")\n",
        "print(\"--- Finished Snippet 5 ---\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T11:34:46.240703Z",
          "iopub.execute_input": "2025-04-23T11:34:46.241027Z",
          "iopub.status.idle": "2025-04-23T11:34:46.276501Z",
          "shell.execute_reply.started": "2025-04-23T11:34:46.240999Z",
          "shell.execute_reply": "2025-04-23T11:34:46.276007Z"
        },
        "id": "iF2scWU2Allu",
        "outputId": "1ca7f3e9-f0fc-4b98-ae9e-f2f27691bc02"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n--- Running Snippet 5: Defining Main Rewrite & Verify Logic ---\n\nDefined rewrite_and_verify function.\n--- Finished Snippet 5 ---\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Snippet 6: Example Usage (Expanded with Diverse S1 Test Cases - S1 INDEX AWARE) ---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This snippet demonstrates how to use the core `rewrite_and_verify` function defined in Snippet 5. It sets up a series of diverse test cases, specifically tailored to events within **Season 1 of 'The Wire'**, reflecting the assumed content of the loaded ChromaDB index. It then runs each case through the spoiler detection and rewriting process, logs the results, measures performance, and provides a final summary.\n",
        "\n",
        "## Key Actions\n",
        "\n",
        "1.  **Prerequisite Check:**\n",
        "    *   Before running any tests, it verifies that all preceding setup steps were successful by checking the global flags: `INSTALL_SUCCESS`, `EMBEDDING_LOAD_SUCCESS`, `INDEX_LOAD_SUCCESS`, and `LLM_LOAD_SUCCESS`.\n",
        "    *   If any of these flags are `False`, it skips the example usage section and prints a message, preventing errors due to missing dependencies.\n",
        "\n",
        "2.  **Define Test Cases (`test_cases`):**\n",
        "    *   A list of dictionaries, where each dictionary represents a specific test scenario.\n",
        "    *   **Structure:** Each dictionary contains:\n",
        "        *   `id` (str): A unique identifier for the test case (e.g., \"Case S1-01\").\n",
        "        *   `text` (str): The input text containing potential spoilers to be processed.\n",
        "        *   `user_progress` (str): The user's viewing progress (e.g., \"S1E2\"), used for spoiler timeline comparison.\n",
        "        *   `expected_outcome_comment` (str): A comment indicating whether the test *should* ideally PASS or FAIL final verification, *based on the assumption that the knowledge base (ChromaDB index) only contains Season 1 information*.\n",
        "    *   **S1 Index Awareness:** The test cases are deliberately crafted using events primarily from Season 1. This aligns testing with the expected scope of the `the_wire_s1_chroma_db` index.\n",
        "    *   **Diversity:** Includes various scenarios:\n",
        "        *   Users at different stages (early, mid, late S1).\n",
        "        *   Clear spoilers from later S1 episodes.\n",
        "        *   Text containing only past events (should pass).\n",
        "        *   Subtle character arc spoilers within S1.\n",
        "        *   Name drops (should pass if no future plot revealed).\n",
        "        *   **Known Limitation Cases (S1-11, S1-12):** Includes text mentioning events from future seasons (S2, S3, S5). These are *expected to PASS* because the system's knowledge base (the S1 index) lacks the information needed to identify these as spoilers. This demonstrates the boundary of the system's current knowledge.\n",
        "    *   **Reused Scenarios:** Some cases reuse text snippets with different user progress points to test the system's sensitivity to the `user_progress` parameter.\n",
        "\n",
        "3.  **Execute Test Cases:**\n",
        "    *   Iterates through the `test_cases` list.\n",
        "    *   **Logging & Formatting:** Prints clear separators (`---`, `>>>`, etc.) and headers for each test case, including its ID, user progress, and expected outcome, making the console output easier to follow.\n",
        "    *   **Call Main Logic:** For each case, it calls the `rewrite_and_verify(text, progress)` function.\n",
        "    *   **Timing:** Measures the execution time for each individual test case using `time.time()`.\n",
        "    *   **Store Results:** Stores the outcome (`success` boolean), the `final_text` produced, the `original` text, the `expected_comment`, and the `duration_seconds` in a `results` dictionary, keyed by the `case_id`.\n",
        "    *   **Display Case Result:** Prints the final PASS/FAIL status for the case, its duration, the original text, and the potentially rewritten `final_text`.\n",
        "\n",
        "4.  **Print Summary:**\n",
        "    *   After processing all test cases, it prints a summary section.\n",
        "    *   **Formatting:** Uses prominent separators (`###`) to clearly mark the summary block.\n",
        "    *   **Content:** Displays:\n",
        "        *   The PASS/FAIL status and duration for each individual case alongside its expected outcome comment.\n",
        "        *   Total number of cases run.\n",
        "        *   Counts of cases that passed vs. failed the final verification.\n",
        "        *   The sum of individual case durations.\n",
        "        *   The total wall-clock time for running all tests.\n",
        "\n",
        "## Context & Importance\n",
        "\n",
        "This snippet serves as the primary validation and demonstration component.\n",
        "*   It **tests the end-to-end functionality** by integrating the RAG (`retrieve_and_filter_context`), LLM (`generate_text_unsloth`), and orchestration logic (`rewrite_and_verify`).\n",
        "*   It provides **concrete examples** of how the system handles different types of potential spoilers relative to user progress.\n",
        "*   The **S1-aware design** makes the results meaningful within the context of the limited (Season 1 only) knowledge base, highlighting both capabilities and limitations (e.g., inability to detect S2+ spoilers).\n",
        "*   **Performance measurement** gives insights into the computational cost of the iterative rewrite/verify process.\n",
        "*   The detailed logging and summary allow for quick assessment of the system's accuracy and behavior across various scenarios. It's essential for debugging, evaluation, and understanding the system's strengths and weaknesses."
      ],
      "metadata": {
        "id": "z99zBPRqAllu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Snippet 6: Example Usage (Expanded with Diverse S1 Test Cases - S1 INDEX AWARE) ===\n",
        "print(\"\\n--- Running Snippet 6: Example Usage (Expanded - S1 INDEX AWARE) ---\")\n",
        "\n",
        "# Ensure all components are ready before running tests\n",
        "if not (INSTALL_SUCCESS and EMBEDDING_LOAD_SUCCESS and INDEX_LOAD_SUCCESS and LLM_LOAD_SUCCESS):\n",
        "    print(\"Skipping example usage due to earlier setup errors.\")\n",
        "else:\n",
        "    # Define test cases (using the provided S1 focused list)\n",
        "    test_cases = [\n",
        "        # --- Basic Setup & Early Spoilers (Focus: S1 Accuracy) ---\n",
        "        {\n",
        "            \"id\": \"Case S1-01 (Very Early User, Mid-Season Spoilers)\",\n",
        "            \"text\": \"After Brandon is tortured and killed for robbing the stash, Omar vows revenge. This leads him to kill Stinkum later on.\",\n",
        "            \"user_progress\": \"S1E2\", # User is before Brandon's death (S1E4) and Stinkum's death (S1E5).\n",
        "            \"expected_outcome_comment\": \"Should FAIL (Contains S1E4/E5 spoilers)\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"Case S1-02 (Early User, Past Events Only)\",\n",
        "            \"text\": \"The investigation kicks off when McNulty talks to Judge Phelan about the Barksdale crew following the Gant murder trial where D'Angelo got off.\",\n",
        "            \"user_progress\": \"S1E3\", # User is past these initial events (S1E1).\n",
        "            \"expected_outcome_comment\": \"Should PASS (All events are S1E1, user is past)\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"Case S1-03 (Mid-Season User, Late Season Spoilers)\",\n",
        "            \"text\": \"Things escalate drastically when Kima Greggs is shot during an undercover buy operation, putting the entire detail in jeopardy.\",\n",
        "            \"user_progress\": \"S1E6\", # User is well before Kima's shooting (S1E10).\n",
        "            \"expected_outcome_comment\": \"Should FAIL (Contains S1E10 spoiler)\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"Case S1-04 (Mid-Season User, Mixed Past/Future)\",\n",
        "            \"text\": \"D'Angelo teaches Wallace and Bodie chess in the low-rises. Sadly, Wallace's story ends tragically when he's killed by his friends on Stringer's orders.\",\n",
        "            \"user_progress\": \"S1E5\", # User is past chess (S1E3) but before Wallace's death (S1E12).\n",
        "            \"expected_outcome_comment\": \"Should FAIL (Contains S1E12 spoiler for Wallace)\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"Case S1-05 (Mid-Season User, Past Events Only)\",\n",
        "            \"text\": \"The detail faces bureaucratic hurdles getting equipment, while Herc, Carver, and Prez make early mistakes. Freamon quietly proves his skills finding D'Angelo's picture.\",\n",
        "            \"user_progress\": \"S1E7\", # User is past these early/mid-season events (approx E1-E4).\n",
        "            \"expected_outcome_comment\": \"Should PASS (Events are early S1, user is past)\"\n",
        "        },\n",
        "        # --- Late Season Spoilers & Nuance (Focus: S1 Accuracy) ---\n",
        "        {\n",
        "            \"id\": \"Case S1-06 (Late User, Finale Spoilers)\",\n",
        "            \"text\": \"In the season finale, Avon Barksdale is arrested, but on lesser charges, while Stringer Bell walks free. Wee-Bey takes the fall for multiple murders.\",\n",
        "            \"user_progress\": \"S1E10\", # User is before the final arrests and outcomes (S1E13).\n",
        "            \"expected_outcome_comment\": \"Should FAIL (Contains S1E13 spoilers)\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"Case S1-07 (Late User, Specific Detail Spoiler)\",\n",
        "            \"text\": \"The crucial break comes when the detail successfully clones the Barksdale crew's pagers, allowing them to track messages despite the changing codes.\",\n",
        "            \"user_progress\": \"S1E5\", # User is before the pager cloning works (around S1E6/E7).\n",
        "            \"expected_outcome_comment\": \"Should FAIL (Contains S1E6/E7 spoiler)\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"Case S1-08 (Late User, Subtle Character Arc Spoiler)\",\n",
        "            \"text\": \"Wallace becomes disillusioned with the game and tries to leave, but his actions ultimately lead to fatal consequences.\",\n",
        "            \"user_progress\": \"S1E8\", # User is before Wallace leaves (S1E9) & death (S1E12).\n",
        "            \"expected_outcome_comment\": \"Should FAIL (Hints strongly at S1E12 spoiler)\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"Case S1-09 (Very Late User, Past Events Only)\",\n",
        "            \"text\": \"Reflecting on the season, key moments included Omar testifying against Bird in court and the detail's failed sting attempt at Orlando's.\",\n",
        "            \"user_progress\": \"S1E13\", # User has finished the season. Orlando's (S1E7), Bird Trial (S1E11).\n",
        "            \"expected_outcome_comment\": \"Should PASS (Events are past for user)\"\n",
        "        },\n",
        "        # --- Edge Cases & Future Season Spoilers (Focus: S1 INDEX LIMITATION) ---\n",
        "        {\n",
        "            \"id\": \"Case S1-10 (Edge Case - Name Drop)\",\n",
        "            \"text\": \"The complex web of alliances involves figures like Proposition Joe, who runs a separate East side crew.\",\n",
        "            \"user_progress\": \"S1E6\", # Prop Joe appears briefly early on. Name alone isn't a spoiler.\n",
        "            \"expected_outcome_comment\": \"Should PASS (Name drop within S1, not spoiler)\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"Case S1-11 (Future Season 2 Spoiler)\",\n",
        "            \"text\": \"While Season 1 focuses on the Barksdales, the investigation later shifts focus to the Baltimore port system and the Sobotka family.\",\n",
        "            \"user_progress\": \"S1E12\", # Mentions S2 characters/plot.\n",
        "            \"expected_outcome_comment\": \"Should PASS (S2 info NOT in S1 index, system cannot verify)\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"Case S1-12 (Very Distant Future Spoiler)\",\n",
        "            \"text\": \"Stringer Bell eventually tries to go legitimate but is ultimately killed by Omar and Brother Mouzone. Later, Marlo Stanfield rises to power.\",\n",
        "            \"user_progress\": \"S1E4\", # Mentions S3/S5 events.\n",
        "            \"expected_outcome_comment\": \"Should PASS (S3/S5 info NOT in S1 index, system cannot verify)\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"Case S1-13 (Heavy Multi-Spoiler - Reuse)\",\n",
        "            \"text\": \"The season culminates tragically with Wallace killed by Bodie and Poot under Stringer's orders, Kima shot during a buy, and D'Angelo ultimately taking a long prison sentence to protect Avon and the family.\",\n",
        "            \"user_progress\": \"S1E2\", # Events are S1E10-S1E13.\n",
        "            \"expected_outcome_comment\": \"Should FAIL (Multiple late S1 spoilers)\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"Case S1-14 (Past & Future Spoilers - Reuse)\",\n",
        "            \"text\": \"After Gant's murder early on, Omar gets revenge for Brandon by killing Stinkum in broad daylight, which forces Stringer to change pager codes.\",\n",
        "            \"user_progress\": \"S1E4\", # Gant (S1E1) is past. Stinkum (S1E5)/Pagers (S1E6) are future.\n",
        "            \"expected_outcome_comment\": \"Should FAIL (Contains S1E5/E6 spoilers)\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"Case S1-15 (Past Spoilers Only - Reuse)\",\n",
        "            \"text\": \"Reviewing early events, we saw McNulty trigger the investigation via Judge Phelan, and D'Angelo teach Wallace and Bodie chess during downtime in the Pit.\",\n",
        "            \"user_progress\": \"S1E5\", # Phelan (S1E1)/Chess (S1E3) are past.\n",
        "            \"expected_outcome_comment\": \"Should PASS (Events are past for user)\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"Case S1-16 (Character Fate - D'Angelo)\",\n",
        "            \"text\": \"D'Angelo struggles with his conscience throughout the season, a conflict that eventually leads to his imprisonment at the end.\",\n",
        "            \"user_progress\": \"S1E9\", # Imprisonment outcome is S1E13.\n",
        "            \"expected_outcome_comment\": \"Should FAIL (Contains S1E13 character fate spoiler)\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # --- Run Test Cases ---\n",
        "    results = {}\n",
        "    start_time_all_tests = time.time() # Time all tests\n",
        "\n",
        "    for i, case in enumerate(test_cases):\n",
        "        case_id = case[\"id\"]\n",
        "        text = case[\"text\"]\n",
        "        progress = case[\"user_progress\"]\n",
        "        expected_comment = case.get(\"expected_outcome_comment\", \"N/A\")\n",
        "\n",
        "        # <<< CHANGE: Add separator line and extra newline before starting a case\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(f\"--- Running Test Case {i+1}/{len(test_cases)}: {case_id} (User at {progress}) ---\")\n",
        "        print(f\" (Expected based on S1 Index: {expected_comment})\")\n",
        "        print(\"-\"*80 + \"\\n\") # <<< CHANGE: Add separator line after starting header\n",
        "\n",
        "        start_time_case = time.time()\n",
        "        # Assuming rewrite_and_verify and supporting functions are defined above\n",
        "        final_text, success = rewrite_and_verify(text, progress)\n",
        "        end_time_case = time.time()\n",
        "        case_duration = end_time_case - start_time_case\n",
        "\n",
        "        results[case_id] = {\n",
        "            \"success\": success,\n",
        "            \"final_text\": final_text,\n",
        "            \"original\": text,\n",
        "            \"expected_comment\": expected_comment,\n",
        "            \"duration_seconds\": round(case_duration, 2)\n",
        "            }\n",
        "\n",
        "        result_status = \"PASSED\" if success else \"FAILED\"\n",
        "        # <<< CHANGE: Add newline before printing final result block\n",
        "        print(f\"\\n>>> FINAL RESULT Block ({case_id}): {result_status} FINAL VERIFICATION ({case_duration:.2f} seconds)\")\n",
        "        print(\"-\" * 40) # <<< CHANGE: Add a sub-separator\n",
        "\n",
        "        print(\"Original Text:\")\n",
        "        print(text)\n",
        "        print(\"-\" * 20) # Keep original separator\n",
        "        print(f\"Final Text ({result_status}):\")\n",
        "        print(final_text)\n",
        "        print(\"-\" * 40) # <<< CHANGE: Add a sub-separator\n",
        "        print(f\"--- Finished {case_id} ---\")\n",
        "        # <<< CHANGE: Add extra newline after finishing a case (before next case's header)\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "    end_time_all_tests = time.time()\n",
        "    total_duration = end_time_all_tests - start_time_all_tests\n",
        "\n",
        "    # --- Optional: Print Summary ---\n",
        "    # <<< CHANGE: Add separator before summary\n",
        "    print(\"\\n\" + \"#\"*80)\n",
        "    print(\"#\" + \" \" * 24 + \"Test Case Summary (S1 Index Aware)\" + \" \" * 24 + \"#\")\n",
        "    print(\"#\"*80 + \"\\n\")\n",
        "\n",
        "    passed_count = 0\n",
        "    failed_count = 0\n",
        "    total_duration_reported = 0.0\n",
        "    for case_id, result in results.items():\n",
        "        status = \"PASSED\" if result[\"success\"] else \"FAILED\"\n",
        "        expected = result[\"expected_comment\"]\n",
        "        duration = result[\"duration_seconds\"]\n",
        "        total_duration_reported += duration # Sum durations from results\n",
        "        print(f\"{case_id}: {status} ({duration}s) (Expected: {expected})\")\n",
        "        if result[\"success\"]: passed_count += 1\n",
        "        else: failed_count += 1\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Total Cases Run: {len(test_cases)}\")\n",
        "    print(f\"Passed Final Verification: {passed_count}\")\n",
        "    print(f\"Failed Final Verification: {failed_count}\")\n",
        "    # <<< CHANGE: Report summed duration from results for potentially better accuracy if tests run fast\n",
        "    print(f\"Sum of Individual Case Durations: {total_duration_reported:.2f} seconds\")\n",
        "    print(f\"Total Test Execution Time (Wall Clock): {total_duration:.2f} seconds\")\n",
        "    print(\"--- End of Summary ---\")\n",
        "    # <<< CHANGE: Add separator after summary\n",
        "    print(\"\\n\" + \"#\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Finished Snippet 6 (Expanded - S1 Index Aware) ---\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T12:11:58.277137Z",
          "iopub.execute_input": "2025-04-23T12:11:58.277416Z",
          "iopub.status.idle": "2025-04-23T12:17:44.030510Z",
          "shell.execute_reply.started": "2025-04-23T12:11:58.277396Z",
          "shell.execute_reply": "2025-04-23T12:17:44.029679Z"
        },
        "id": "9MrlDmcXAllu",
        "outputId": "a1ac9c8d-7ab6-488a-d36b-284787b20841"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n--- Running Snippet 6: Example Usage (Expanded - S1 INDEX AWARE) ---\n\n--------------------------------------------------------------------------------\n--- Running Test Case 1/16: Case S1-01 (Very Early User, Mid-Season Spoilers) (User at S1E2) ---\n (Expected based on S1 Index: Should FAIL (Contains S1E4/E5 spoilers))\n--------------------------------------------------------------------------------\n\n\n=== Starting Spoiler Rewriting for User at S1E2 ===\nOriginal Text (first 150 chars): After Brandon is tortured and killed for robbing the stash, Omar vows revenge. This leads him to kill Stinkum later on....\nSafe Context: Retrieved 0 items about past episodes (up to S1E2)\nInitial RAG (threshold 0.58): Found 12 potential spoilers.\n\n--- Progressive Refinement: Attempt 1/4 ---\nUsing verification threshold: 0.58\n>> Calling Verifier LLM for current candidate...\nVerifier Result: FAIL: \"After Brandon is tortured and killed for robbing the stash, Omar vows revenge. This leads him to kill Stinkum later on.\"  (This reveals events ...\n>> Running secondary verification to prevent hallucination...\nSecondary verification result: CONFIRMED SPOILER\nVerification identified issues. Feedback: \"After Brandon is tortured and killed for robbing the stash, Omar vows revenge. This leads him to ki...\n>> Calling Rewriter LLM for targeted rewriting...\nRewriter Output (first 150 chars): After Brandon is tortured and killed for robbing the stash, Omar vows revenge. This leads him to seek retribution against those involved....\n\n--- Progressive Refinement: Attempt 2/4 ---\nUsing verification threshold: 0.54\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions Omar seeking revenge for Brandon's death and targeting those involved. While this sets up future conflict, it doesn't explici...\nPhase 2 verification passed, proceeding to stricter phase 3\n\n--- Progressive Refinement: Attempt 3/4 ---\nUsing verification threshold: 0.50\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text focuses on Omar's desire for revenge after Brandon's death. While it mentions future events (Omar seeking retribution), it doesn't exp...\nPhase 3 verification passed, proceeding to stricter phase 4\n\n--- Progressive Refinement: Attempt 4/4 ---\nUsing verification threshold: 0.46\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text focuses on Omar's desire for revenge after Brandon's death. While it mentions future events (seeking retribution), it doesn't explicit...\n\n=== Verification PASSED on final attempt 4 ===\n\n>>> FINAL RESULT Block (Case S1-01 (Very Early User, Mid-Season Spoilers)): PASSED FINAL VERIFICATION (23.83 seconds)\n----------------------------------------\nOriginal Text:\nAfter Brandon is tortured and killed for robbing the stash, Omar vows revenge. This leads him to kill Stinkum later on.\n--------------------\nFinal Text (PASSED):\nAfter Brandon is tortured and killed for robbing the stash, Omar vows revenge. This leads him to seek retribution against those involved.\n----------------------------------------\n--- Finished Case S1-01 (Very Early User, Mid-Season Spoilers) ---\n\n\n\n--------------------------------------------------------------------------------\n--- Running Test Case 2/16: Case S1-02 (Early User, Past Events Only) (User at S1E3) ---\n (Expected based on S1 Index: Should PASS (All events are S1E1, user is past))\n--------------------------------------------------------------------------------\n\n\n=== Starting Spoiler Rewriting for User at S1E3 ===\nOriginal Text (first 150 chars): The investigation kicks off when McNulty talks to Judge Phelan about the Barksdale crew following the Gant murder trial where D'Angelo got off....\nSafe Context: Retrieved 4 items about past episodes (up to S1E3)\nInitial RAG (threshold 0.58): Found 3 potential spoilers.\n\n--- Progressive Refinement: Attempt 1/4 ---\nUsing verification threshold: 0.58\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions events that occur within Season 1, specifically the Gant murder trial and D'Angelo's acquittal.  These are all events that ha...\nPhase 1 verification passed, proceeding to stricter phase 2\n\n--- Progressive Refinement: Attempt 2/4 ---\nUsing verification threshold: 0.54\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions events that occur within Season 1, Episode 3 or earlier.  It references the Gant murder trial and D'Angelo getting off, which...\nPhase 2 verification passed, proceeding to stricter phase 3\n\n--- Progressive Refinement: Attempt 3/4 ---\nUsing verification threshold: 0.50\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text only refers to events that occur up to and including Season 1, Episode 3.  It mentions the Gant murder trial and D'Angelo getting off,...\nPhase 3 verification passed, proceeding to stricter phase 4\n\n--- Progressive Refinement: Attempt 4/4 ---\nUsing verification threshold: 0.46\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text only refers to events that occur up to and including Season 1, Episode 3.  It mentions the Gant murder trial and D'Angelo getting off,...\n\n=== Verification PASSED on final attempt 4 ===\n\n>>> FINAL RESULT Block (Case S1-02 (Early User, Past Events Only)): PASSED FINAL VERIFICATION (21.36 seconds)\n----------------------------------------\nOriginal Text:\nThe investigation kicks off when McNulty talks to Judge Phelan about the Barksdale crew following the Gant murder trial where D'Angelo got off.\n--------------------\nFinal Text (PASSED):\nThe investigation kicks off when McNulty talks to Judge Phelan about the Barksdale crew following the Gant murder trial where D'Angelo got off.\n----------------------------------------\n--- Finished Case S1-02 (Early User, Past Events Only) ---\n\n\n\n--------------------------------------------------------------------------------\n--- Running Test Case 3/16: Case S1-03 (Mid-Season User, Late Season Spoilers) (User at S1E6) ---\n (Expected based on S1 Index: Should FAIL (Contains S1E10 spoiler))\n--------------------------------------------------------------------------------\n\n\n=== Starting Spoiler Rewriting for User at S1E6 ===\nOriginal Text (first 150 chars): Things escalate drastically when Kima Greggs is shot during an undercover buy operation, putting the entire detail in jeopardy....\nSafe Context: Retrieved 1 items about past episodes (up to S1E6)\nInitial RAG (threshold 0.58): Found 6 potential spoilers.\n\n--- Progressive Refinement: Attempt 1/4 ---\nUsing verification threshold: 0.58\n>> Calling Verifier LLM for current candidate...\nVerifier Result: FAIL: \"Things escalate drastically when Kima Greggs is shot during an undercover buy operation, putting the entire detail in jeopardy.\"  (This spoils ...\n>> Running secondary verification to prevent hallucination...\nSecondary verification result: CONFIRMED SPOILER\nVerification identified issues. Feedback: \"Things escalate drastically when Kima Greggs is shot during an undercover buy operation, putting th...\n>> Calling Rewriter LLM for targeted rewriting...\nRewriter Output (first 150 chars): Challenges arise for the detail as they face unexpected obstacles in their investigation....\n\n--- Progressive Refinement: Attempt 2/4 ---\nUsing verification threshold: 0.54\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text \"Challenges arise for the detail as they face unexpected obstacles in their investigation\" is generic and could apply to many situatio...\nPhase 2 verification passed, proceeding to stricter phase 3\n\n--- Progressive Refinement: Attempt 3/4 ---\nUsing verification threshold: 0.50\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text \"Challenges arise for the detail as they face unexpected obstacles in their investigation\" is a general statement that could apply to ...\nPhase 3 verification passed, proceeding to stricter phase 4\n\n--- Progressive Refinement: Attempt 4/4 ---\nUsing verification threshold: 0.46\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text \"Challenges arise for the detail as they face unexpected obstacles in their investigation\" is a general statement that could apply to ...\n\n=== Verification PASSED on final attempt 4 ===\n\n>>> FINAL RESULT Block (Case S1-03 (Mid-Season User, Late Season Spoilers)): PASSED FINAL VERIFICATION (23.73 seconds)\n----------------------------------------\nOriginal Text:\nThings escalate drastically when Kima Greggs is shot during an undercover buy operation, putting the entire detail in jeopardy.\n--------------------\nFinal Text (PASSED):\nChallenges arise for the detail as they face unexpected obstacles in their investigation.\n----------------------------------------\n--- Finished Case S1-03 (Mid-Season User, Late Season Spoilers) ---\n\n\n\n--------------------------------------------------------------------------------\n--- Running Test Case 4/16: Case S1-04 (Mid-Season User, Mixed Past/Future) (User at S1E5) ---\n (Expected based on S1 Index: Should FAIL (Contains S1E12 spoiler for Wallace))\n--------------------------------------------------------------------------------\n\n\n=== Starting Spoiler Rewriting for User at S1E5 ===\nOriginal Text (first 150 chars): D'Angelo teaches Wallace and Bodie chess in the low-rises. Sadly, Wallace's story ends tragically when he's killed by his friends on Stringer's orders...\nSafe Context: Retrieved 3 items about past episodes (up to S1E5)\nInitial RAG (threshold 0.58): Found 9 potential spoilers.\n\n--- Progressive Refinement: Attempt 1/4 ---\nUsing verification threshold: 0.58\n>> Calling Verifier LLM for current candidate...\nFAIL: \"Sadly, Wallace's story ends tragically when he's killed by his friends on Stringer's orders.\" (S1E12)\n>> Running secondary verification to prevent hallucination...\nSecondary verification result: CONFIRMED SPOILER\nVerification identified issues. Feedback: \"Sadly, Wallace's story ends tragically when he's killed by his friends on Stringer's orders.\" (S1E1...\n>> Calling Rewriter LLM for targeted rewriting...\nRewriter Output (first 150 chars): D'Angelo teaches Wallace and Bodie chess in the low-rises. Wallace's life takes a difficult turn as he navigates the complexities of the Barksdale org...\n\n--- Progressive Refinement: Attempt 2/4 ---\nUsing verification threshold: 0.54\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions D'Angelo teaching Wallace and Bodie chess and Wallace's life taking a difficult turn. These events are consistent with the es...\nPhase 2 verification passed, proceeding to stricter phase 3\n\n--- Progressive Refinement: Attempt 3/4 ---\nUsing verification threshold: 0.50\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text discusses events that are consistent with what happens up to and including S1E5.  \n\n* **D'Angelo teaching Wallace and Bodie chess:** T...\nPhase 3 verification passed, proceeding to stricter phase 4\n\n--- Progressive Refinement: Attempt 4/4 ---\nUsing verification threshold: 0.46\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text discusses events that are consistent with what happens up to and including Season 1, Episode 5.  It mentions D'Angelo teaching Wallace...\n\n=== Verification PASSED on final attempt 4 ===\n\n>>> FINAL RESULT Block (Case S1-04 (Mid-Season User, Mixed Past/Future)): PASSED FINAL VERIFICATION (30.08 seconds)\n----------------------------------------\nOriginal Text:\nD'Angelo teaches Wallace and Bodie chess in the low-rises. Sadly, Wallace's story ends tragically when he's killed by his friends on Stringer's orders.\n--------------------\nFinal Text (PASSED):\nD'Angelo teaches Wallace and Bodie chess in the low-rises. Wallace's life takes a difficult turn as he navigates the complexities of the Barksdale organization.\n----------------------------------------\n--- Finished Case S1-04 (Mid-Season User, Mixed Past/Future) ---\n\n\n\n--------------------------------------------------------------------------------\n--- Running Test Case 5/16: Case S1-05 (Mid-Season User, Past Events Only) (User at S1E7) ---\n (Expected based on S1 Index: Should PASS (Events are early S1, user is past))\n--------------------------------------------------------------------------------\n\n\n=== Starting Spoiler Rewriting for User at S1E7 ===\nOriginal Text (first 150 chars): The detail faces bureaucratic hurdles getting equipment, while Herc, Carver, and Prez make early mistakes. Freamon quietly proves his skills finding D...\nSafe Context: Retrieved 3 items about past episodes (up to S1E7)\nInitial RAG (threshold 0.58): Found 9 potential spoilers.\n\n--- Progressive Refinement: Attempt 1/4 ---\nUsing verification threshold: 0.58\n>> Calling Verifier LLM for current candidate...\nPASS \n\n\nThe text provided does not contain any spoilers for episodes beyond Season 1, Episode 7.\nPhase 1 verification passed, proceeding to stricter phase 2\n\n--- Progressive Refinement: Attempt 2/4 ---\nUsing verification threshold: 0.54\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\n\nThe text provided only mentions events and character actions that are consistent with what has happened up to and including Season 1, Episode ...\nPhase 2 verification passed, proceeding to stricter phase 3\n\n--- Progressive Refinement: Attempt 3/4 ---\nUsing verification threshold: 0.50\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text only mentions events and character actions that are consistent with what has happened up to and including Season 1, Episode 7.  There ...\nPhase 3 verification passed, proceeding to stricter phase 4\n\n--- Progressive Refinement: Attempt 4/4 ---\nUsing verification threshold: 0.46\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text only mentions events and character actions that are consistent with what has happened up to and including Season 1, Episode 7.  There ...\n\n=== Verification PASSED on final attempt 4 ===\n\n>>> FINAL RESULT Block (Case S1-05 (Mid-Season User, Past Events Only)): PASSED FINAL VERIFICATION (17.60 seconds)\n----------------------------------------\nOriginal Text:\nThe detail faces bureaucratic hurdles getting equipment, while Herc, Carver, and Prez make early mistakes. Freamon quietly proves his skills finding D'Angelo's picture.\n--------------------\nFinal Text (PASSED):\nThe detail faces bureaucratic hurdles getting equipment, while Herc, Carver, and Prez make early mistakes. Freamon quietly proves his skills finding D'Angelo's picture.\n----------------------------------------\n--- Finished Case S1-05 (Mid-Season User, Past Events Only) ---\n\n\n\n--------------------------------------------------------------------------------\n--- Running Test Case 6/16: Case S1-06 (Late User, Finale Spoilers) (User at S1E10) ---\n (Expected based on S1 Index: Should FAIL (Contains S1E13 spoilers))\n--------------------------------------------------------------------------------\n\n\n=== Starting Spoiler Rewriting for User at S1E10 ===\nOriginal Text (first 150 chars): In the season finale, Avon Barksdale is arrested, but on lesser charges, while Stringer Bell walks free. Wee-Bey takes the fall for multiple murders....\nSafe Context: Retrieved 6 items about past episodes (up to S1E10)\nInitial RAG (threshold 0.58): Found 8 potential spoilers.\n\n--- Progressive Refinement: Attempt 1/4 ---\nUsing verification threshold: 0.58\n>> Calling Verifier LLM for current candidate...\nVerifier Result: FAIL: \"In the season finale, Avon Barksdale is arrested, but on lesser charges, while Stringer Bell walks free. Wee-Bey takes the fall for multiple mu...\n>> Running secondary verification to prevent hallucination...\nSecondary verification result: CONFIRMED SPOILER\nVerification identified issues. Feedback: \"In the season finale, Avon Barksdale is arrested, but on lesser charges, while Stringer Bell walks ...\n>> Calling Rewriter LLM for targeted rewriting...\nRewriter Output (first 150 chars): In the season finale, Avon Barksdale faces legal consequences, while Stringer Bell's situation remains uncertain. Wee-Bey Brice finds himself in a dif...\n\n--- Progressive Refinement: Attempt 2/4 ---\nUsing verification threshold: 0.54\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions Avon Barksdale facing legal consequences and Stringer Bell's situation remaining uncertain, which are all consistent with eve...\nPhase 2 verification passed, proceeding to stricter phase 3\n\n--- Progressive Refinement: Attempt 3/4 ---\nUsing verification threshold: 0.50\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text discusses Avon Barksdale facing legal consequences and Stringer Bell's uncertain situation, which are all consistent with the events l...\nPhase 3 verification passed, proceeding to stricter phase 4\n\n--- Progressive Refinement: Attempt 4/4 ---\nUsing verification threshold: 0.46\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text discusses Avon Barksdale facing legal consequences and Stringer Bell's uncertain situation, which are all consistent with events that ...\n\n=== Verification PASSED on final attempt 4 ===\n\n>>> FINAL RESULT Block (Case S1-06 (Late User, Finale Spoilers)): PASSED FINAL VERIFICATION (30.58 seconds)\n----------------------------------------\nOriginal Text:\nIn the season finale, Avon Barksdale is arrested, but on lesser charges, while Stringer Bell walks free. Wee-Bey takes the fall for multiple murders.\n--------------------\nFinal Text (PASSED):\nIn the season finale, Avon Barksdale faces legal consequences, while Stringer Bell's situation remains uncertain. Wee-Bey Brice finds himself in a difficult position.\n----------------------------------------\n--- Finished Case S1-06 (Late User, Finale Spoilers) ---\n\n\n\n--------------------------------------------------------------------------------\n--- Running Test Case 7/16: Case S1-07 (Late User, Specific Detail Spoiler) (User at S1E5) ---\n (Expected based on S1 Index: Should FAIL (Contains S1E6/E7 spoiler))\n--------------------------------------------------------------------------------\n\n\n=== Starting Spoiler Rewriting for User at S1E5 ===\nOriginal Text (first 150 chars): The crucial break comes when the detail successfully clones the Barksdale crew's pagers, allowing them to track messages despite the changing codes....\nSafe Context: Retrieved 6 items about past episodes (up to S1E5)\nInitial RAG (threshold 0.58): Found 1 potential spoilers.\n\n--- Progressive Refinement: Attempt 1/4 ---\nUsing verification threshold: 0.58\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\n\nThe text mentions the detail successfully cloning the Barksdale crew's pagers and tracking messages despite changing codes. This is consistent...\nPhase 1 verification passed, proceeding to stricter phase 2\n\n--- Progressive Refinement: Attempt 2/4 ---\nUsing verification threshold: 0.54\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions the detail successfully cloning the Barksdale crew's pagers and tracking messages despite changing codes. This event could po...\nPhase 2 verification passed, proceeding to stricter phase 3\n\n--- Progressive Refinement: Attempt 3/4 ---\nUsing verification threshold: 0.50\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text describes the detail successfully cloning the Barksdale crew's pagers and tracking messages despite changing codes. This event is cons...\nPhase 3 verification passed, proceeding to stricter phase 4\n\n--- Progressive Refinement: Attempt 4/4 ---\nUsing verification threshold: 0.46\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text describes the detail successfully cloning the Barksdale crew's pagers and tracking messages. This event is consistent with the events ...\n\n=== Verification PASSED on final attempt 4 ===\n\n>>> FINAL RESULT Block (Case S1-07 (Late User, Specific Detail Spoiler)): PASSED FINAL VERIFICATION (21.60 seconds)\n----------------------------------------\nOriginal Text:\nThe crucial break comes when the detail successfully clones the Barksdale crew's pagers, allowing them to track messages despite the changing codes.\n--------------------\nFinal Text (PASSED):\nThe crucial break comes when the detail successfully clones the Barksdale crew's pagers, allowing them to track messages despite the changing codes.\n----------------------------------------\n--- Finished Case S1-07 (Late User, Specific Detail Spoiler) ---\n\n\n\n--------------------------------------------------------------------------------\n--- Running Test Case 8/16: Case S1-08 (Late User, Subtle Character Arc Spoiler) (User at S1E8) ---\n (Expected based on S1 Index: Should FAIL (Hints strongly at S1E12 spoiler))\n--------------------------------------------------------------------------------\n\n\n=== Starting Spoiler Rewriting for User at S1E8 ===\nOriginal Text (first 150 chars): Wallace becomes disillusioned with the game and tries to leave, but his actions ultimately lead to fatal consequences....\nSafe Context: Retrieved 2 items about past episodes (up to S1E8)\nInitial RAG (threshold 0.58): Found 4 potential spoilers.\n\n--- Progressive Refinement: Attempt 1/4 ---\nUsing verification threshold: 0.58\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text states that Wallace \"tries to leave\" and his actions have \"fatal consequences\". While this alludes to a tragic ending, it doesn't expl...\nPhase 1 verification passed, proceeding to stricter phase 2\n\n--- Progressive Refinement: Attempt 2/4 ---\nUsing verification threshold: 0.54\n>> Calling Verifier LLM for current candidate...\nVerifier Result: FAIL: \"Wallace becomes disillusioned with the game and tries to leave, but his actions ultimately lead to fatal consequences.\"  \n\nThis statement revea...\n>> Running secondary verification to prevent hallucination...\nSecondary verification result: CONFIRMED SPOILER\nVerification identified issues. Feedback: \"Wallace becomes disillusioned with the game and tries to leave, but his actions ultimately lead to ...\n>> Calling Rewriter LLM for targeted rewriting...\nRewriter Output (first 150 chars): Wallace experiences conflicting feelings about his involvement in the drug trade and contemplates a different path, but his choices lead to unforeseen...\n\n--- Progressive Refinement: Attempt 3/4 ---\nUsing verification threshold: 0.50\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text discusses Wallace's internal conflict and potential for change, which are themes explored throughout Season 1.  It doesn't explicitly ...\nPhase 3 verification passed, proceeding to stricter phase 4\n\n--- Progressive Refinement: Attempt 4/4 ---\nUsing verification threshold: 0.46\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text discusses Wallace's internal conflict and potential for change, which are themes explored throughout Season 1. While it mentions \"unfo...\n\n=== Verification PASSED on final attempt 4 ===\n\n>>> FINAL RESULT Block (Case S1-08 (Late User, Subtle Character Arc Spoiler)): PASSED FINAL VERIFICATION (24.13 seconds)\n----------------------------------------\nOriginal Text:\nWallace becomes disillusioned with the game and tries to leave, but his actions ultimately lead to fatal consequences.\n--------------------\nFinal Text (PASSED):\nWallace experiences conflicting feelings about his involvement in the drug trade and contemplates a different path, but his choices lead to unforeseen and difficult circumstances.\n----------------------------------------\n--- Finished Case S1-08 (Late User, Subtle Character Arc Spoiler) ---\n\n\n\n--------------------------------------------------------------------------------\n--- Running Test Case 9/16: Case S1-09 (Very Late User, Past Events Only) (User at S1E13) ---\n (Expected based on S1 Index: Should PASS (Events are past for user))\n--------------------------------------------------------------------------------\n\n\n=== Starting Spoiler Rewriting for User at S1E13 ===\nOriginal Text (first 150 chars): Reflecting on the season, key moments included Omar testifying against Bird in court and the detail's failed sting attempt at Orlando's....\nSafe Context: Retrieved 8 items about past episodes (up to S1E13)\nInitial RAG (threshold 0.58): Found 0 potential spoilers.\n\n--- Progressive Refinement: Attempt 1/4 ---\nUsing verification threshold: 0.58\n>> Calling Verifier LLM for current candidate...\nPASS \n\nThe text only mentions events that occur within Season 1.\nPhase 1 verification passed, proceeding to stricter phase 2\n\n--- Progressive Refinement: Attempt 2/4 ---\nUsing verification threshold: 0.54\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions events that occur within Season 1:\n\n* **Omar testifying against Bird:** This happens in the Season 1 finale (S1E13).\n* **The ...\nPhase 2 verification passed, proceeding to stricter phase 3\n\n--- Progressive Refinement: Attempt 3/4 ---\nUsing verification threshold: 0.50\n>> Calling Verifier LLM for current candidate...\nPASS \n\nThe text only mentions events that occur within Season 1.\nPhase 3 verification passed, proceeding to stricter phase 4\n\n--- Progressive Refinement: Attempt 4/4 ---\nUsing verification threshold: 0.46\n>> Calling Verifier LLM for current candidate...\nPASS \n\nThe text only mentions events that occur within Season 1.\n\n=== Verification PASSED on final attempt 4 ===\n\n>>> FINAL RESULT Block (Case S1-09 (Very Late User, Past Events Only)): PASSED FINAL VERIFICATION (14.65 seconds)\n----------------------------------------\nOriginal Text:\nReflecting on the season, key moments included Omar testifying against Bird in court and the detail's failed sting attempt at Orlando's.\n--------------------\nFinal Text (PASSED):\nReflecting on the season, key moments included Omar testifying against Bird in court and the detail's failed sting attempt at Orlando's.\n----------------------------------------\n--- Finished Case S1-09 (Very Late User, Past Events Only) ---\n\n\n\n--------------------------------------------------------------------------------\n--- Running Test Case 10/16: Case S1-10 (Edge Case - Name Drop) (User at S1E6) ---\n (Expected based on S1 Index: Should PASS (Name drop within S1, not spoiler))\n--------------------------------------------------------------------------------\n\n\n=== Starting Spoiler Rewriting for User at S1E6 ===\nOriginal Text (first 150 chars): The complex web of alliances involves figures like Proposition Joe, who runs a separate East side crew....\nSafe Context: Retrieved 0 items about past episodes (up to S1E6)\nInitial RAG (threshold 0.58): Found 0 potential spoilers.\n\n--- Progressive Refinement: Attempt 1/4 ---\nUsing verification threshold: 0.58\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions \"Proposition Joe\" who is a character introduced in Season 1.  There is no explicit mention of future events or anything that ...\nPhase 1 verification passed, proceeding to stricter phase 2\n\n--- Progressive Refinement: Attempt 2/4 ---\nUsing verification threshold: 0.54\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions \"Proposition Joe\" who is a character introduced in Season 1.  There is no explicit mention of future events or developments b...\nPhase 2 verification passed, proceeding to stricter phase 3\n\n--- Progressive Refinement: Attempt 3/4 ---\nUsing verification threshold: 0.50\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions \"Proposition Joe\" who is a character introduced in Season 1.  There is no indication of future events beyond what has already...\nPhase 3 verification passed, proceeding to stricter phase 4\n\n--- Progressive Refinement: Attempt 4/4 ---\nUsing verification threshold: 0.46\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions \"Proposition Joe\" who is a character introduced in Season 1.  There is no indication of future events beyond what has already...\n\n=== Verification PASSED on final attempt 4 ===\n\n>>> FINAL RESULT Block (Case S1-10 (Edge Case - Name Drop)): PASSED FINAL VERIFICATION (16.68 seconds)\n----------------------------------------\nOriginal Text:\nThe complex web of alliances involves figures like Proposition Joe, who runs a separate East side crew.\n--------------------\nFinal Text (PASSED):\nThe complex web of alliances involves figures like Proposition Joe, who runs a separate East side crew.\n----------------------------------------\n--- Finished Case S1-10 (Edge Case - Name Drop) ---\n\n\n\n--------------------------------------------------------------------------------\n--- Running Test Case 11/16: Case S1-11 (Future Season 2 Spoiler) (User at S1E12) ---\n (Expected based on S1 Index: Should PASS (S2 info NOT in S1 index, system cannot verify))\n--------------------------------------------------------------------------------\n\n\n=== Starting Spoiler Rewriting for User at S1E12 ===\nOriginal Text (first 150 chars): While Season 1 focuses on the Barksdales, the investigation later shifts focus to the Baltimore port system and the Sobotka family....\nSafe Context: Retrieved 6 items about past episodes (up to S1E12)\nInitial RAG (threshold 0.58): Found 2 potential spoilers.\n\n--- Progressive Refinement: Attempt 1/4 ---\nUsing verification threshold: 0.58\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text states that the investigation \"later shifts focus to the Baltimore port system and the Sobotka family.\" While this does hint at future...\nPhase 1 verification passed, proceeding to stricter phase 2\n\n--- Progressive Refinement: Attempt 2/4 ---\nUsing verification threshold: 0.54\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text states that the investigation \"later shifts focus to the Baltimore port system and the Sobotka family.\" While this does hint at future...\nPhase 2 verification passed, proceeding to stricter phase 3\n\n--- Progressive Refinement: Attempt 3/4 ---\nUsing verification threshold: 0.50\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions the port system and the Sobotka family, which are indeed central to later seasons. However, it doesn't explicitly reveal any ...\nPhase 3 verification passed, proceeding to stricter phase 4\n\n--- Progressive Refinement: Attempt 4/4 ---\nUsing verification threshold: 0.46\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions the Sobotka family and the Baltimore port system, which are indeed central to later seasons. However, it doesn't explicitly r...\n\n=== Verification PASSED on final attempt 4 ===\n\n>>> FINAL RESULT Block (Case S1-11 (Future Season 2 Spoiler)): PASSED FINAL VERIFICATION (24.36 seconds)\n----------------------------------------\nOriginal Text:\nWhile Season 1 focuses on the Barksdales, the investigation later shifts focus to the Baltimore port system and the Sobotka family.\n--------------------\nFinal Text (PASSED):\nWhile Season 1 focuses on the Barksdales, the investigation later shifts focus to the Baltimore port system and the Sobotka family.\n----------------------------------------\n--- Finished Case S1-11 (Future Season 2 Spoiler) ---\n\n\n\n--------------------------------------------------------------------------------\n--- Running Test Case 12/16: Case S1-12 (Very Distant Future Spoiler) (User at S1E4) ---\n (Expected based on S1 Index: Should PASS (S3/S5 info NOT in S1 index, system cannot verify))\n--------------------------------------------------------------------------------\n\n\n=== Starting Spoiler Rewriting for User at S1E4 ===\nOriginal Text (first 150 chars): Stringer Bell eventually tries to go legitimate but is ultimately killed by Omar and Brother Mouzone. Later, Marlo Stanfield rises to power....\nSafe Context: Retrieved 1 items about past episodes (up to S1E4)\nInitial RAG (threshold 0.58): Found 5 potential spoilers.\n\n--- Progressive Refinement: Attempt 1/4 ---\nUsing verification threshold: 0.58\n>> Calling Verifier LLM for current candidate...\nVerifier Result: FAIL: \"Stringer Bell eventually tries to go legitimate but is ultimately killed by Omar and Brother Mouzone. Later, Marlo Stanfield rises to power.\" \n...\n>> Running secondary verification to prevent hallucination...\nSecondary verification result: CONFIRMED SPOILER\nVerification identified issues. Feedback: \"Stringer Bell eventually tries to go legitimate but is ultimately killed by Omar and Brother Mouzon...\n>> Calling Rewriter LLM for targeted rewriting...\nRewriter Output (first 150 chars): Stringer Bell explores different paths in life but faces significant challenges.  Marlo Stanfield becomes increasingly influential....\n\n--- Progressive Refinement: Attempt 2/4 ---\nUsing verification threshold: 0.54\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions general character developments (\"Stringer Bell explores different paths in life but faces significant challenges\" and \"Marlo ...\nPhase 2 verification passed, proceeding to stricter phase 3\n\n--- Progressive Refinement: Attempt 3/4 ---\nUsing verification threshold: 0.50\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions Stringer Bell exploring different paths and Marlo Stanfield becoming influential. These are general statements that could occ...\nPhase 3 verification passed, proceeding to stricter phase 4\n\n--- Progressive Refinement: Attempt 4/4 ---\nUsing verification threshold: 0.46\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions Stringer Bell exploring different paths and Marlo Stanfield becoming influential. These are general statements that could occ...\n\n=== Verification PASSED on final attempt 4 ===\n\n>>> FINAL RESULT Block (Case S1-12 (Very Distant Future Spoiler)): PASSED FINAL VERIFICATION (24.44 seconds)\n----------------------------------------\nOriginal Text:\nStringer Bell eventually tries to go legitimate but is ultimately killed by Omar and Brother Mouzone. Later, Marlo Stanfield rises to power.\n--------------------\nFinal Text (PASSED):\nStringer Bell explores different paths in life but faces significant challenges.  Marlo Stanfield becomes increasingly influential.\n----------------------------------------\n--- Finished Case S1-12 (Very Distant Future Spoiler) ---\n\n\n\n--------------------------------------------------------------------------------\n--- Running Test Case 13/16: Case S1-13 (Heavy Multi-Spoiler - Reuse) (User at S1E2) ---\n (Expected based on S1 Index: Should FAIL (Multiple late S1 spoilers))\n--------------------------------------------------------------------------------\n\n\n=== Starting Spoiler Rewriting for User at S1E2 ===\nOriginal Text (first 150 chars): The season culminates tragically with Wallace killed by Bodie and Poot under Stringer's orders, Kima shot during a buy, and D'Angelo ultimately taking...\nSafe Context: Retrieved 0 items about past episodes (up to S1E2)\nInitial RAG (threshold 0.58): Found 6 potential spoilers.\n\n--- Progressive Refinement: Attempt 1/4 ---\nUsing verification threshold: 0.58\n>> Calling Verifier LLM for current candidate...\nVerifier Result: FAIL: \"The season culminates tragically with Wallace killed by Bodie and Poot under Stringer's orders, Kima shot during a buy, and D'Angelo ultimately...\n>> Running secondary verification to prevent hallucination...\nSecondary verification result: CONFIRMED SPOILER\nVerification identified issues. Feedback: \"The season culminates tragically with Wallace killed by Bodie and Poot under Stringer's orders, Kim...\n>> Calling Rewriter LLM for targeted rewriting...\nRewriter Output (first 150 chars): The season takes some unexpected turns involving Wallace, Bodie, Poot, Stringer, Kima, and D'Angelo....\n\n--- Progressive Refinement: Attempt 2/4 ---\nUsing verification threshold: 0.54\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text simply states that the season has \"unexpected turns\" involving certain characters. It does not explicitly reveal any events that happe...\nPhase 2 verification passed, proceeding to stricter phase 3\n\n--- Progressive Refinement: Attempt 3/4 ---\nUsing verification threshold: 0.50\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text simply states that the season has unexpected turns involving certain characters. This does not reveal any specific plot points or even...\nPhase 3 verification passed, proceeding to stricter phase 4\n\n--- Progressive Refinement: Attempt 4/4 ---\nUsing verification threshold: 0.46\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions characters who appear in Season 1, and states that the season has \"unexpected turns.\" This is a general statement and does no...\n\n=== Verification PASSED on final attempt 4 ===\n\n>>> FINAL RESULT Block (Case S1-13 (Heavy Multi-Spoiler - Reuse)): PASSED FINAL VERIFICATION (23.63 seconds)\n----------------------------------------\nOriginal Text:\nThe season culminates tragically with Wallace killed by Bodie and Poot under Stringer's orders, Kima shot during a buy, and D'Angelo ultimately taking a long prison sentence to protect Avon and the family.\n--------------------\nFinal Text (PASSED):\nThe season takes some unexpected turns involving Wallace, Bodie, Poot, Stringer, Kima, and D'Angelo.\n----------------------------------------\n--- Finished Case S1-13 (Heavy Multi-Spoiler - Reuse) ---\n\n\n\n--------------------------------------------------------------------------------\n--- Running Test Case 14/16: Case S1-14 (Past & Future Spoilers - Reuse) (User at S1E4) ---\n (Expected based on S1 Index: Should FAIL (Contains S1E5/E6 spoilers))\n--------------------------------------------------------------------------------\n\n\n=== Starting Spoiler Rewriting for User at S1E4 ===\nOriginal Text (first 150 chars): After Gant's murder early on, Omar gets revenge for Brandon by killing Stinkum in broad daylight, which forces Stringer to change pager codes....\nSafe Context: Retrieved 0 items about past episodes (up to S1E4)\nInitial RAG (threshold 0.58): Found 6 potential spoilers.\n\n--- Progressive Refinement: Attempt 1/4 ---\nUsing verification threshold: 0.58\n>> Calling Verifier LLM for current candidate...\nPASS \n\nThe text mentions events that occur within Season 1, Episode 4.  It does not reveal any plot points from episodes beyond S1E4.\nPhase 1 verification passed, proceeding to stricter phase 2\n\n--- Progressive Refinement: Attempt 2/4 ---\nUsing verification threshold: 0.54\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions events that occur within Season 1, Episode 4.  It does not reveal any plot points or character developments from episodes bey...\nPhase 2 verification passed, proceeding to stricter phase 3\n\n--- Progressive Refinement: Attempt 3/4 ---\nUsing verification threshold: 0.50\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text mentions events that occur within Season 1, Episode 4.  It does not reveal any plot points or character actions from episodes beyond S...\nPhase 3 verification passed, proceeding to stricter phase 4\n\n--- Progressive Refinement: Attempt 4/4 ---\nUsing verification threshold: 0.46\n>> Calling Verifier LLM for current candidate...\nPASS \n\nThe text only discusses events that occur within Season 1, Episode 4 or earlier.\n\n=== Verification PASSED on final attempt 4 ===\n\n>>> FINAL RESULT Block (Case S1-14 (Past & Future Spoilers - Reuse)): PASSED FINAL VERIFICATION (14.68 seconds)\n----------------------------------------\nOriginal Text:\nAfter Gant's murder early on, Omar gets revenge for Brandon by killing Stinkum in broad daylight, which forces Stringer to change pager codes.\n--------------------\nFinal Text (PASSED):\nAfter Gant's murder early on, Omar gets revenge for Brandon by killing Stinkum in broad daylight, which forces Stringer to change pager codes.\n----------------------------------------\n--- Finished Case S1-14 (Past & Future Spoilers - Reuse) ---\n\n\n\n--------------------------------------------------------------------------------\n--- Running Test Case 15/16: Case S1-15 (Past Spoilers Only - Reuse) (User at S1E5) ---\n (Expected based on S1 Index: Should PASS (Events are past for user))\n--------------------------------------------------------------------------------\n\n\n=== Starting Spoiler Rewriting for User at S1E5 ===\nOriginal Text (first 150 chars): Reviewing early events, we saw McNulty trigger the investigation via Judge Phelan, and D'Angelo teach Wallace and Bodie chess during downtime in the P...\nSafe Context: Retrieved 4 items about past episodes (up to S1E5)\nInitial RAG (threshold 0.58): Found 2 potential spoilers.\n\n--- Progressive Refinement: Attempt 1/4 ---\nUsing verification threshold: 0.58\n>> Calling Verifier LLM for current candidate...\nPASS \n\n\nThe text only mentions events that occur up to and including S1E5.\nPhase 1 verification passed, proceeding to stricter phase 2\n\n--- Progressive Refinement: Attempt 2/4 ---\nUsing verification threshold: 0.54\n>> Calling Verifier LLM for current candidate...\nPASS \n\nThe text only mentions events that occur within Season 1, Episode 5 or earlier.\nPhase 2 verification passed, proceeding to stricter phase 3\n\n--- Progressive Refinement: Attempt 3/4 ---\nUsing verification threshold: 0.50\n>> Calling Verifier LLM for current candidate...\nPASS \n\nThe text only mentions events that occur up to and including Season 1, Episode 5.\nPhase 3 verification passed, proceeding to stricter phase 4\n\n--- Progressive Refinement: Attempt 4/4 ---\nUsing verification threshold: 0.46\n>> Calling Verifier LLM for current candidate...\nPASS \n\nThe text only mentions events that occur within Season 1, Episode 5 or earlier.\n\n=== Verification PASSED on final attempt 4 ===\n\n>>> FINAL RESULT Block (Case S1-15 (Past Spoilers Only - Reuse)): PASSED FINAL VERIFICATION (11.64 seconds)\n----------------------------------------\nOriginal Text:\nReviewing early events, we saw McNulty trigger the investigation via Judge Phelan, and D'Angelo teach Wallace and Bodie chess during downtime in the Pit.\n--------------------\nFinal Text (PASSED):\nReviewing early events, we saw McNulty trigger the investigation via Judge Phelan, and D'Angelo teach Wallace and Bodie chess during downtime in the Pit.\n----------------------------------------\n--- Finished Case S1-15 (Past Spoilers Only - Reuse) ---\n\n\n\n--------------------------------------------------------------------------------\n--- Running Test Case 16/16: Case S1-16 (Character Fate - D'Angelo) (User at S1E9) ---\n (Expected based on S1 Index: Should FAIL (Contains S1E13 character fate spoiler))\n--------------------------------------------------------------------------------\n\n\n=== Starting Spoiler Rewriting for User at S1E9 ===\nOriginal Text (first 150 chars): D'Angelo struggles with his conscience throughout the season, a conflict that eventually leads to his imprisonment at the end....\nSafe Context: Retrieved 6 items about past episodes (up to S1E9)\nInitial RAG (threshold 0.58): Found 2 potential spoilers.\n\n--- Progressive Refinement: Attempt 1/4 ---\nUsing verification threshold: 0.58\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text states \"a conflict that eventually leads to his imprisonment at the end\". While this implies D'Angelo's fate, it doesn't explicitly re...\nPhase 1 verification passed, proceeding to stricter phase 2\n\n--- Progressive Refinement: Attempt 2/4 ---\nUsing verification threshold: 0.54\n>> Calling Verifier LLM for current candidate...\nFAIL: \"a conflict that eventually leads to his imprisonment at the end\" (This spoils D'Angelo's fate in later seasons)\n>> Running secondary verification to prevent hallucination...\nSecondary verification result: CONFIRMED SPOILER\nVerification identified issues. Feedback: \"a conflict that eventually leads to his imprisonment at the end\" (This spoils D'Angelo's fate in la...\n>> Calling Rewriter LLM for targeted rewriting...\nRewriter Output (first 150 chars): D'Angelo struggles with his conscience throughout the season, facing difficult choices that shape his future....\n\n--- Progressive Refinement: Attempt 3/4 ---\nUsing verification threshold: 0.50\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text \"D'Angelo struggles with his conscience throughout the season, facing difficult choices that shape his future\"  is a general statement...\nPhase 3 verification passed, proceeding to stricter phase 4\n\n--- Progressive Refinement: Attempt 4/4 ---\nUsing verification threshold: 0.46\n>> Calling Verifier LLM for current candidate...\nVerifier Result: PASS \n\nThe text states that D'Angelo struggles with his conscience and faces difficult choices that shape his future. This is consistent with his char...\n\n=== Verification PASSED on final attempt 4 ===\n\n>>> FINAL RESULT Block (Case S1-16 (Character Fate - D'Angelo)): PASSED FINAL VERIFICATION (22.75 seconds)\n----------------------------------------\nOriginal Text:\nD'Angelo struggles with his conscience throughout the season, a conflict that eventually leads to his imprisonment at the end.\n--------------------\nFinal Text (PASSED):\nD'Angelo struggles with his conscience throughout the season, facing difficult choices that shape his future.\n----------------------------------------\n--- Finished Case S1-16 (Character Fate - D'Angelo) ---\n\n\n\n################################################################################\n#                        Test Case Summary (S1 Index Aware)                        #\n################################################################################\n\nCase S1-01 (Very Early User, Mid-Season Spoilers): PASSED (23.83s) (Expected: Should FAIL (Contains S1E4/E5 spoilers))\nCase S1-02 (Early User, Past Events Only): PASSED (21.36s) (Expected: Should PASS (All events are S1E1, user is past))\nCase S1-03 (Mid-Season User, Late Season Spoilers): PASSED (23.73s) (Expected: Should FAIL (Contains S1E10 spoiler))\nCase S1-04 (Mid-Season User, Mixed Past/Future): PASSED (30.08s) (Expected: Should FAIL (Contains S1E12 spoiler for Wallace))\nCase S1-05 (Mid-Season User, Past Events Only): PASSED (17.6s) (Expected: Should PASS (Events are early S1, user is past))\nCase S1-06 (Late User, Finale Spoilers): PASSED (30.58s) (Expected: Should FAIL (Contains S1E13 spoilers))\nCase S1-07 (Late User, Specific Detail Spoiler): PASSED (21.6s) (Expected: Should FAIL (Contains S1E6/E7 spoiler))\nCase S1-08 (Late User, Subtle Character Arc Spoiler): PASSED (24.13s) (Expected: Should FAIL (Hints strongly at S1E12 spoiler))\nCase S1-09 (Very Late User, Past Events Only): PASSED (14.65s) (Expected: Should PASS (Events are past for user))\nCase S1-10 (Edge Case - Name Drop): PASSED (16.68s) (Expected: Should PASS (Name drop within S1, not spoiler))\nCase S1-11 (Future Season 2 Spoiler): PASSED (24.36s) (Expected: Should PASS (S2 info NOT in S1 index, system cannot verify))\nCase S1-12 (Very Distant Future Spoiler): PASSED (24.44s) (Expected: Should PASS (S3/S5 info NOT in S1 index, system cannot verify))\nCase S1-13 (Heavy Multi-Spoiler - Reuse): PASSED (23.63s) (Expected: Should FAIL (Multiple late S1 spoilers))\nCase S1-14 (Past & Future Spoilers - Reuse): PASSED (14.68s) (Expected: Should FAIL (Contains S1E5/E6 spoilers))\nCase S1-15 (Past Spoilers Only - Reuse): PASSED (11.64s) (Expected: Should PASS (Events are past for user))\nCase S1-16 (Character Fate - D'Angelo): PASSED (22.75s) (Expected: Should FAIL (Contains S1E13 character fate spoiler))\n------------------------------\nTotal Cases Run: 16\nPassed Final Verification: 16\nFailed Final Verification: 0\nSum of Individual Case Durations: 345.74 seconds\nTotal Test Execution Time (Wall Clock): 345.74 seconds\n--- End of Summary ---\n\n################################################################################\n\n\n--- Finished Snippet 6 (Expanded - S1 Index Aware) ---\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}